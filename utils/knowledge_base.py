# ğŸ“œ saga/utils/knowledge_base.py

import os
import uuid
import re
import pickle
from typing import List, Dict, Any, Optional, Tuple

# --- ç¬¬ä¸‰æ–¹åº“ ---
import chromadb
from chromadb.errors import NotFoundError
from langchain_text_splitters import RecursiveCharacterTextSplitter, MarkdownTextSplitter
from rank_bm25 import BM25Okapi
from langdetect import detect, DetectorFactory
from langdetect.lang_detect_exception import LangDetectException

# --- å†…éƒ¨æ¨¡å— ---
from .config import config
from .logging_config import logger
from .llm_service import llm_service
from .database import db_manager
from .prompt_manager import prompt_manager

# ç¡®ä¿ç»“æœå¯å¤ç°
DetectorFactory.seed = 0

def cut_thinking_txt(text: str) -> str:
    """
    ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ç§»é™¤</think>å‰çš„å†…å®¹ï¼Œä¸“é—¨ç”¨äºå¤„ç†æ¨¡å‹çš„æ€è€ƒè¿‡ç¨‹ã€‚
    """
    if not text: return ""
    
    pattern = r'(.*?)<\/think>'
    result = re.sub(pattern, '', text, flags=re.DOTALL)
    result = re.sub(r'\n+', '\n', result).strip()
    return result

class SmartTextSplitter:
    """
    æ™ºèƒ½æ–‡æœ¬åˆ†å‰²å™¨ï¼Œæ ¹æ®æ–‡æ¡£ç±»å‹é‡‡ç”¨ä¸åŒç­–ç•¥ï¼Œå¹¶æºå¸¦å…ƒæ•°æ®ã€‚

    RAGä¼˜åŒ–ç‰¹æ€§ï¼š
    1. æ–‡æ¡£ç±»å‹æ„ŸçŸ¥ï¼šPDFã€Markdownã€æ™®é€šæ–‡æœ¬é‡‡ç”¨ä¸åŒç­–ç•¥
    2. ç« èŠ‚ç»“æ„ä¿ç•™ï¼šPDFæ–‡æ¡£æŒ‰ç« èŠ‚å±‚çº§åˆ†å‰²
    3. è¯­ä¹‰æ„ŸçŸ¥é‡å ï¼šåœ¨è¯­ä¹‰è¾¹ç•Œå¤„åˆ†å‰²
    4. åŠ¨æ€å—å¤§å°ï¼šæ ¹æ®æ–‡æ¡£ç‰¹å¾åŠ¨æ€è°ƒæ•´
    """
    def __init__(self):
        self.chunk_size = config.get('knowledge_base.chunk_size', 1000)
        self.chunk_overlap = config.get('knowledge_base.chunk_overlap', 150)

        # åŸºç¡€åˆ†å‰²å™¨é…ç½®
        self.general_splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap,
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", "ï¼Œ", " ", ""],
        )
        self.markdown_splitter = MarkdownTextSplitter(
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap,
        )

        # PDFç« èŠ‚è¯†åˆ«æ¨¡å¼
        self.pdf_chapter_patterns = [
            r'^(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡0-9]+[ç« èŠ‚å·ç¯‡éƒ¨]|Chapter\s+\d+|Part\s+\d+)',
            r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡]+[ã€ï¼\.]\s*\S',  # ä¸­æ–‡åºå·
            r'^\d+\.\s+\S+',  # æ•°å­—åºå·
            r'^[A-Z][A-Z0-9]+\s+\S+',  # å¤§å†™å­—æ¯åºå·
        ]

    def split_text(self, text: str, doc_type: str, file_metadata: Dict = None) -> List[Dict[str, Any]]:
        """
        æ ¹æ®æ–‡ä»¶å†…å®¹å’Œè·¯å¾„æ™ºèƒ½åˆ†å‰²æ–‡æœ¬ï¼Œè¿”å›åŒ…å«æ–‡æœ¬å’Œå…ƒæ•°æ®çš„å­—å…¸åˆ—è¡¨ã€‚

        Args:
            text: å¾…åˆ†å‰²æ–‡æœ¬
            doc_type: æ–‡æ¡£ç±»å‹ (pdf, markdown, general)
            file_metadata: æ–‡ä»¶å…ƒæ•°æ®ï¼ˆç”¨äºå¢å¼ºåˆ†å‰²ç­–ç•¥ï¼‰

        Returns:
            åŒ…å« 'text' å’Œ 'metadata' çš„å­—å…¸åˆ—è¡¨
        """
        if not text or not text.strip():
            return []

        # æ¸…ç†æ–‡æœ¬
        text = self._clean_text(text)

        if doc_type == "markdown":
            chunks = self._split_markdown(text)
        elif doc_type == "pdf":
            chunks = self._split_pdf_with_chapters(text, file_metadata)
        else:
            chunks = self._split_general(text)

        # ä¸ºæ¯ä¸ªå—æ·»åŠ å…ƒæ•°æ®
        chunk_dicts = []
        for i, chunk_text in enumerate(chunks):
            metadata = {
                'chunk_index': i,
                'chunk_type': self._detect_chunk_type(chunk_text),
                'doc_type': doc_type,
                'language': self._detect_language(chunk_text)
            }
            # åˆå¹¶æ–‡ä»¶å…ƒæ•°æ®
            if file_metadata:
                metadata.update(file_metadata)

            chunk_dicts.append({
                'text': chunk_text,
                'metadata': metadata
            })

        return chunk_dicts

    def _clean_text(self, text: str) -> str:
        """æ¸…ç†æ–‡æœ¬ï¼šå»é™¤è¿‡å¤šç©ºç™½ã€ä¿®å¤ç¼–ç é—®é¢˜ç­‰"""
        # å»é™¤å¤šä½™çš„ç©ºç™½è¡Œ
        text = re.sub(r'\n{3,}', '\n\n', text)
        # å»é™¤è¡Œé¦–è¡Œå°¾ç©ºç™½
        text = '\n'.join(line.strip() for line in text.split('\n'))
        return text.strip()

    def _split_markdown(self, text: str) -> List[str]:
        """
        åˆ†å‰²Markdownæ–‡æœ¬ï¼Œä¿ç•™æ ‡é¢˜å±‚çº§ç»“æ„

        ä¼˜åŒ–ç­–ç•¥ï¼š
        1. åœ¨æ ‡é¢˜å¤„åˆ†å‰²ï¼Œä¿ç•™ä¸Šä¸‹æ–‡
        2. ç¡®ä¿ä»£ç å—å®Œæ•´æ€§
        3. è¡¨æ ¼ä¸è¢«åˆ†å‰²
        """
        # ä½¿ç”¨Markdownä¸“ç”¨åˆ†å‰²å™¨
        raw_chunks = self.markdown_splitter.split_text(text)

        # åå¤„ç†ï¼šæ£€æŸ¥å¹¶ä¿®å¤è¢«ç ´åçš„ç»“æ„
        chunks = []
        for chunk in raw_chunks:
            # ç¡®ä¿ä»£ç å—å®Œæ•´ï¼ˆæ£€æŸ¥```é…å¯¹ï¼‰
            if chunk.count('```') % 2 == 1:
                # ä»£ç å—ä¸å®Œæ•´ï¼Œå°è¯•ä¸ä¸‹ä¸€å—åˆå¹¶
                continue
            chunks.append(chunk)

        return chunks if chunks else raw_chunks

    def _split_pdf_with_chapters(self, text: str, file_metadata: Dict = None) -> List[str]:
        """
        åˆ†å‰²PDFæ–‡æœ¬ï¼Œä¿ç•™ç« èŠ‚ç»“æ„

        ä¼˜åŒ–ç­–ç•¥ï¼š
        1. è¯†åˆ«ç« èŠ‚æ ‡é¢˜
        2. åœ¨ç« èŠ‚è¾¹ç•Œå¤„ä¼˜å…ˆåˆ†å‰²
        3. åŠ¨æ€è°ƒæ•´å—å¤§å°ä»¥é€‚åº”ç« èŠ‚é•¿åº¦
        4. æ·»åŠ ç« èŠ‚ä¸Šä¸‹æ–‡åˆ°æ¯ä¸ªå—
        """
        lines = text.split('\n')
        chapters = self._identify_chapters(lines)

        if not chapters:
            # æ²¡æœ‰è¯†åˆ«åˆ°ç« èŠ‚ï¼Œä½¿ç”¨é€šç”¨åˆ†å‰²
            return self.general_splitter.split_text(text)

        chunks = []
        for chapter in chapters:
            chapter_title = chapter['title']
            chapter_content = chapter['content']

            # æ ¹æ®ç« èŠ‚é•¿åº¦åŠ¨æ€è°ƒæ•´åˆ†å‰²ç­–ç•¥
            if len(chapter_content) <= self.chunk_size:
                # çŸ­ç« èŠ‚ï¼Œç›´æ¥ä½œä¸ºä¸€ä¸ªå—
                chunk = f"# {chapter_title}\n\n{chapter_content}"
                chunks.append(chunk)
            else:
                # é•¿ç« èŠ‚ï¼Œè¿›ä¸€æ­¥åˆ†å‰²ä½†ä¿ç•™ç« èŠ‚æ ‡é¢˜
                sub_chunks = self.general_splitter.split_text(chapter_content)
                for i, sub_chunk in enumerate(sub_chunks):
                    # æ·»åŠ ç« èŠ‚ä¸Šä¸‹æ–‡
                    context = f"# {chapter_title}"
                    if i > 0:
                        context += f" (ç»­)"
                    chunk = f"{context}\n\n{sub_chunk}"
                    chunks.append(chunk)

        return chunks

    def _identify_chapters(self, lines: List[str]) -> List[Dict[str, Any]]:
        """è¯†åˆ«PDFä¸­çš„ç« èŠ‚ç»“æ„"""
        chapters = []
        current_chapter = {'title': 'å¼•è¨€', 'content': '', 'level': 0}

        for line in lines:
            is_chapter = False
            chapter_level = 0

            # æ£€æŸ¥æ˜¯å¦åŒ¹é…ç« èŠ‚æ¨¡å¼
            for i, pattern in enumerate(self.pdf_chapter_patterns):
                if re.match(pattern, line.strip(), re.IGNORECASE | re.MULTILINE):
                    is_chapter = True
                    chapter_level = i + 1
                    break

            if is_chapter:
                # ä¿å­˜å½“å‰ç« èŠ‚
                if current_chapter['content'].strip():
                    chapters.append(current_chapter.copy())
                # å¼€å§‹æ–°ç« èŠ‚
                current_chapter = {
                    'title': line.strip(),
                    'content': '',
                    'level': chapter_level
                }
            else:
                current_chapter['content'] += line + '\n'

        # æ·»åŠ æœ€åä¸€ä¸ªç« èŠ‚
        if current_chapter['content'].strip():
            chapters.append(current_chapter)

        # å¦‚æœæ²¡æœ‰è¯†åˆ«åˆ°ç« èŠ‚ï¼Œè¿”å›ç©ºåˆ—è¡¨
        if len(chapters) == 1 and chapters[0]['title'] == 'å¼•è¨€':
            return []

        return chapters

    def _split_general(self, text: str) -> List[str]:
        """
        åˆ†å‰²æ™®é€šæ–‡æœ¬ï¼Œä½¿ç”¨è¯­ä¹‰è¾¹ç•Œä¼˜åŒ–

        ä¼˜åŒ–ç­–ç•¥ï¼š
        1. ä¼˜å…ˆåœ¨æ®µè½è¾¹ç•Œåˆ†å‰²
        2. ä¿ç•™å¥å­å®Œæ•´æ€§
        3. ä½¿ç”¨è¯­ä¹‰æ„ŸçŸ¥çš„é‡å 
        """
        return self.general_splitter.split_text(text)

    def _detect_chunk_type(self, chunk_text: str) -> str:
        """æ£€æµ‹æ–‡æœ¬å—ç±»å‹ï¼ˆç”¨äºåç»­æ£€ç´¢ä¼˜åŒ–ï¼‰"""
        if re.search(r'```', chunk_text):
            return 'code'
        elif re.search(r'^#+\s', chunk_text, re.MULTILINE):
            return 'heading'
        elif re.search(r'\|.*\|', chunk_text):
            return 'table'
        elif len(chunk_text.split('\n')) > 5:
            return 'paragraph'
        else:
            return 'short'

    def _detect_language(self, text: str) -> str:
        """æ£€æµ‹æ–‡æœ¬è¯­è¨€"""
        try:
            lang = detect(text[:500])  # åªæ£€æµ‹å‰500å­—ç¬¦
            return lang
        except LangDetectException:
            return 'unknown'

class KnowledgeBaseManager:
    """çŸ¥è¯†åº“ç®¡ç†ç±»ï¼Œæ”¯æŒå¤šç»´åº¦å‘é‡éš”ç¦»ã€æ··åˆæ£€ç´¢(ChromaDB+BM25)ã€å‡è®¾æ€§æ–‡æ¡£åµŒå…¥HyDEã€Rerankerç²¾æ’å’Œä¸Šä¸‹æ–‡æº¯æºã€‚"""
    def __init__(self):
        chroma_db_path = config.get('paths.chroma_db')
        self.bm25_indices_path = config.get('paths.bm25_indices')
        os.makedirs(chroma_db_path, exist_ok=True)
        os.makedirs(self.bm25_indices_path, exist_ok=True)
        
        self.client = chromadb.PersistentClient(path=chroma_db_path)
        self.text_splitter = SmartTextSplitter()
        logger.info(f"ChromaDB å®¢æˆ·ç«¯å·²è¿æ¥åˆ°: {chroma_db_path}")
        logger.info(f"BM25 ç´¢å¼•å°†å­˜æ”¾åœ¨: {self.bm25_indices_path}")
        
        # æ·»åŠ åµŒå…¥ç»´åº¦ç¼“å­˜
        self.embedding_dimensions = {}
        
    def _get_embedding_dimension(self, model_name: str) -> int:
        """è·å–æŒ‡å®šåµŒå…¥æ¨¡å‹çš„å‘é‡ç»´åº¦ æš‚æ—¶æœªä½¿ç”¨"""
        if model_name in self.embedding_dimensions:
            return self.embedding_dimensions[model_name]
        
        # å¸¸è§æ¨¡å‹çš„é¢„è®¾ç»´åº¦
        dimension_map = {
            'text-embedding-v4': 1024,  # é€šä¹‰åƒé—®text-embedding-v4çš„ç»´åº¦ 2,048ã€1,536ã€1,024ï¼ˆé»˜è®¤ï¼‰ã€768ã€512ã€256ã€128ã€64
            'deepseek-text-embedding': 1536,
            'qwen3-embedding:0.6b': 1024,  # Ollamaæ¨¡å‹
            'mxbai-embed-large': 1024,
            'default_internal': 1024,  # å†…éƒ¨æœåŠ¡é»˜è®¤
        }
        
        if model_name in dimension_map:
            self.embedding_dimensions[model_name] = dimension_map[model_name]
        else:
            # å¯¹äºæœªçŸ¥æ¨¡å‹ï¼Œä½¿ç”¨é»˜è®¤å€¼å¹¶å°è¯•åŠ¨æ€è·å–
            logger.warning(f"æœªçŸ¥åµŒå…¥æ¨¡å‹ '{model_name}'ï¼Œä½¿ç”¨é»˜è®¤ç»´åº¦1024")
            self.embedding_dimensions[model_name] = 1024
            
            # å°è¯•åŠ¨æ€è·å–ç»´åº¦ï¼ˆå¯é€‰ï¼‰
            try:
                # å‘é€ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬è·å–å‘é‡ç»´åº¦
                test_embedding = llm_service.get_embedding("test")
                if test_embedding:
                    dimension = len(test_embedding)
                    self.embedding_dimensions[model_name] = dimension
                    logger.info(f"åŠ¨æ€æ£€æµ‹åˆ°æ¨¡å‹ '{model_name}' çš„ç»´åº¦ä¸º: {dimension}")
            except Exception as e:
                logger.warning(f"æ— æ³•åŠ¨æ€æ£€æµ‹æ¨¡å‹ '{model_name}' çš„ç»´åº¦: {e}")
        
        return self.embedding_dimensions[model_name]
        
    def _get_bm25_index_path(self, kb_id: int) -> str:
        """è·å–ç‰¹å®šçŸ¥è¯†åº“çš„BM25ç´¢å¼•æ–‡ä»¶çš„è·¯å¾„"""
        return os.path.join(self.bm25_indices_path, f"bm25_kb_{kb_id}.pkl")

    def _rebuild_bm25_index(self, kb_id: int):
        """ã€æ ¸å¿ƒä¼˜åŒ–ã€‘ä»æ•°æ®åº“ç›´æ¥è¯»å–æ–‡æœ¬å—ï¼Œå…¨é‡é‡å»ºæŒ‡å®šçŸ¥è¯†åº“çš„BM25ç´¢å¼•ã€‚"""
        logger.info(f"æ­£åœ¨ä¸ºçŸ¥è¯†åº“ ID {kb_id} é«˜æ•ˆé‡å»º BM25 ç´¢å¼•...")
        
        # 1. ä»æ•°æ®åº“è·å–æ‰€æœ‰chunks
        all_chunks_data = db_manager.get_chunks_by_kb_id(kb_id)
        
        if not all_chunks_data:
            index_path = self._get_bm25_index_path(kb_id)
            if os.path.exists(index_path):
                os.remove(index_path)
            logger.info(f"çŸ¥è¯†åº“ ID {kb_id} ä¸ºç©ºï¼Œå·²æ¸…ç† BM25 ç´¢å¼•ã€‚")
            return
        
        corpus_chunks = [item['chunk_text'] for item in all_chunks_data]
        
        # 2. æ„å»ºå¹¶ä¿å­˜BM25ç´¢å¼•
        tokenized_corpus = [doc.split(" ") for doc in corpus_chunks]
        bm25 = BM25Okapi(tokenized_corpus)
        index_path = self._get_bm25_index_path(kb_id)
        with open(index_path, 'wb') as f:
            # å­˜å‚¨bm25å¯¹è±¡å’ŒåŸå§‹chunksï¼Œç”¨äºBM25æ£€ç´¢æ—¶çš„å†…å®¹è¿”å›
            pickle.dump({'bm25': bm25, 'corpus': corpus_chunks}, f)
        
        logger.info(f"BM25 ç´¢å¼•å·²ä¸ºçŸ¥è¯†åº“ ID {kb_id} æˆåŠŸå…¨é‡é‡å»ºï¼ŒåŒ…å« {len(corpus_chunks)} ä¸ªæ–‡æœ¬å—ã€‚")

    def get_or_create_collection(self, name: str) -> chromadb.Collection:
        """è·å–æˆ–åˆ›å»ºä¸€ä¸ªChromaDBé›†åˆ"""
        return self.client.get_or_create_collection(name=name)
    
    def _translate_if_needed(self, text: str) -> str:
        """æ£€æµ‹æ–‡æœ¬è¯­è¨€ï¼Œå¦‚æœä¸æ˜¯ä¸­æ–‡ï¼Œåˆ™è°ƒç”¨LLMè¿›è¡Œç¿»è¯‘ã€‚"""
        if not text or not text.strip():
            return ""

        # 1. é¢„å¤„ç†ï¼šç§»é™¤ä»£ç å—ã€URLã€Markdownç‰¹æ®Šå­—ç¬¦ï¼Œå‡å°‘å¹²æ‰°
        # ç§»é™¤Markdownä»£ç å—
        processed_text = re.sub(r'```.*?```', '', text, flags=re.DOTALL)
        # ç§»é™¤URL
        processed_text = re.sub(r'https?://\S+', '', processed_text)
        # ç§»é™¤Markdownå›¾ç‰‡å’Œé“¾æ¥
        processed_text = re.sub(r'!\[.*?\]\(.*?\)|\[.*?\]\(.*?\)', '', processed_text)
        # ç§»é™¤Markdownæ ‡é¢˜æ ‡è®°
        processed_text = re.sub(r'#+\s', '', processed_text)
        # ç§»é™¤HTMLæ ‡ç­¾
        processed_text = re.sub(r'<.*?>', '', processed_text)
        
        # åªå–å‰1000ä¸ªå­—ç¬¦è¿›è¡Œæ£€æµ‹ï¼Œæé«˜æ•ˆç‡å’Œå‡†ç¡®æ€§
        sample_text = processed_text.strip()[:1000]

        if not sample_text:
            logger.info("æ–‡æœ¬é¢„å¤„ç†åä¸ºç©ºï¼Œè·³è¿‡ç¿»è¯‘ã€‚")
            return text

        try:
            # 2. ä½¿ç”¨ langdetect è¿›è¡Œæ£€æµ‹
            language = detect(sample_text)
            # æ”¯æŒç®€ä½“(zh-cn)å’Œç¹ä½“(zh-tw)
            if language.startswith('zh'):
                logger.info(f"è¯­è¨€æ£€æµ‹ä¸º '{language}'ï¼Œè·³è¿‡ç¿»è¯‘ã€‚")
                return text
        except LangDetectException:
            # å¦‚æœæ–‡æœ¬å¤ªçŸ­æˆ–å¤ªæ¨¡ç³Šæ— æ³•æ£€æµ‹ï¼Œé»˜è®¤ä¸ç¿»è¯‘
            logger.warning("æ— æ³•æ˜ç¡®æ£€æµ‹æ–‡æœ¬è¯­è¨€ï¼Œå°†ä½¿ç”¨åŸæ–‡ã€‚")
            return text

        logger.info(f"æ£€æµ‹åˆ°è¯­è¨€ä¸º '{language}'ï¼Œå¼€å§‹ç¿»è¯‘...")
        try:
            translation_prompt = prompt_manager.render('translate_to_chinese.jinja2', text_to_translate=text)
            messages = [{"role": "user", "content": translation_prompt}]
            
            # ä½¿ç”¨ä¸€ä¸ªç‹¬ç«‹çš„ã€è½»é‡çš„llm_serviceè°ƒç”¨
            translated_text, _ = llm_service.chat_completion(messages, topic_id=0, temperature=0.1)  # topic_id 0 for non-session tasks
            
            if translated_text:
                logger.info("æ–‡æœ¬ç¿»è¯‘æˆåŠŸã€‚")
                return cut_thinking_txt(translated_text)
            else:
                logger.warning("ç¿»è¯‘è¿”å›ç©ºå†…å®¹ï¼Œå°†ä½¿ç”¨åŸæ–‡ã€‚")
                return text
        except Exception as e:
            logger.error(f"ç¿»è¯‘è¿‡ç¨‹ä¸­å‡ºé”™: {e}ï¼Œå°†ä½¿ç”¨åŸæ–‡ã€‚", exc_info=True)
            return text

    def add_document(self, file_path: str, kb_id: int, file_id: int):
        """è°ƒç”¨ç»Ÿä¸€çš„llm_serviceæ¥å£æ¥å¤„ç†æ–‡ä»¶å¤„ç†å•ä¸ªæ–‡æ¡£å¹¶å°†å…¶æ·»åŠ åˆ°ä¸å½“å‰æ¨¡å‹åŒ¹é…çš„çŸ¥è¯†åº“é›†åˆä¸­ã€‚"""
        active_embedding_model = llm_service.get_active_embedding_model_name()
        chroma_collection_name = f"kb_{kb_id}_{active_embedding_model}"
        
        logger.info(f"å¼€å§‹æ·»åŠ æ–‡æ¡£ '{os.path.basename(file_path)}' åˆ°é›†åˆ '{chroma_collection_name}'")
        
        try:
            db_manager.update_file_status(file_id, 'processing')
            
            # 1. è°ƒç”¨ llm_service çš„ç»Ÿä¸€æ¥å£
            extraction_result = llm_service.extract_text_from_file(file_path)
            if not extraction_result or not extraction_result.get("text"):
                raise ValueError("ä»æ–‡ä»¶ä¸­æœªèƒ½æå–åˆ°ä»»ä½•æ–‡æœ¬ã€‚")
            
            text = extraction_result["text"]
            
            # translated_text = self._translate_if_needed(text)
            
            doc_type = extraction_result["doc_type"]
            logger.info(f"æˆåŠŸæå–æ–‡æœ¬ï¼Œæ–‡æ¡£ç±»å‹: {doc_type}, é•¿åº¦: {len(text)} å­—ç¬¦ã€‚")

            # 2. åˆ†å‰²æ–‡æœ¬ ä½¿ç”¨ç¿»è¯‘åçš„æ–‡æœ¬è¿›è¡Œåˆ†å‰²
            chunk_dicts = self.text_splitter.split_text(text, doc_type)
            if not chunk_dicts: raise ValueError("æ–‡æœ¬åˆ†å‰²åæœªäº§ç”Ÿä»»ä½•ç‰‡æ®µã€‚")

            # SmartTextSplitter è¿”å›å­—å…¸åˆ—è¡¨: [{'text': ..., 'metadata': ...}, ...]
            # æå–çº¯æ–‡æœ¬åˆ—è¡¨ç”¨äºæ•°æ®åº“å’Œå‘é‡åŒ–
            chunk_texts = [chunk['text'] for chunk in chunk_dicts]

            # å°†chunk_textså­˜å…¥æ•°æ®åº“
            db_manager.add_chunks_to_file(file_id, chunk_texts)

            # 3. å‘é‡åŒ–å’Œå­˜å‚¨
            embeddings = llm_service.get_embedding(chunk_texts)
            if not embeddings: raise RuntimeError("è·å–åµŒå…¥å‘é‡å¤±è´¥ã€‚")

            collection = self.client.get_or_create_collection(chroma_collection_name)
            ids = [str(uuid.uuid4()) for _ in chunk_texts]

            # åˆå¹¶å…ƒæ•°æ®ï¼šåŸºç¡€ä¿¡æ¯ + SmartTextSplitter æä¾›çš„å…ƒæ•°æ®
            base_name = os.path.basename(file_path)
            metadatas = []
            for i, chunk_dict in enumerate(chunk_dicts):
                metadata = {
                    'source': base_name,
                    'file_id': file_id,
                    'chunk_index': i
                }
                # åˆå¹¶åˆ†å‰²å™¨æä¾›çš„å…ƒæ•°æ®
                if 'metadata' in chunk_dict:
                    metadata.update(chunk_dict['metadata'])
                metadatas.append(metadata)

            collection.add(ids=ids, embeddings=embeddings, documents=chunk_texts, metadatas=metadatas)
            
            # 4. å¢é‡æ›´æ–°BM25ç´¢å¼•(ä»æ•°æ®åº“è¯»å–ï¼Œä¿è¯ä¸€è‡´æ€§)
            self._rebuild_bm25_index(kb_id)

            db_manager.update_file_status(file_id, 'completed', vector_count=len(chunk_texts))
            logger.info(f"æˆåŠŸå°† {len(chunk_texts)} ä¸ªå‘é‡å­˜å…¥é›†åˆ '{chroma_collection_name}'ã€‚")

        except Exception as e:
            logger.error(f"æ·»åŠ æ–‡æ¡£ '{file_path}' å¤±è´¥: {e}", exc_info=True)
            db_manager.update_file_status(file_id, 'failed')
            # å¦‚æœè¿‡ç¨‹ä¸­æ–­ï¼Œæœ€å¥½ä¹Ÿé‡å»ºä¸€æ¬¡BM25ç´¢å¼•ä»¥ä¿è¯ä¸€è‡´æ€§
            self._rebuild_bm25_index(kb_id)
            
    def delete_document(self, kb_id: int, file_id: int, embedding_model: str):
        """æ ¹æ® file_id ä»æŒ‡å®šçš„ ChromaDB é›†åˆå’Œ BM25 ç´¢å¼•ä¸­åˆ é™¤æ‰€æœ‰ç›¸å…³çš„å‘é‡ã€‚"""
        chroma_collection_name = f"kb_{kb_id}_{embedding_model}"
        logger.info(f"å‡†å¤‡ä»é›†åˆ '{chroma_collection_name}' ä¸­åˆ é™¤ file_id ä¸º {file_id} çš„æ‰€æœ‰å‘é‡ã€‚")
        try:
            collection = self.client.get_collection(name=chroma_collection_name)
            # ä½¿ç”¨ where filter è¿›è¡Œç²¾ç¡®åˆ é™¤
            collection.delete(where={"file_id": file_id})
            logger.info(f"æˆåŠŸä»é›†åˆ '{chroma_collection_name}' ä¸­åˆ é™¤ä¸ file_id {file_id} ç›¸å…³çš„æ‰€æœ‰å‘é‡ã€‚")

        except NotFoundError:
            # é›†åˆä¸å­˜åœ¨ï¼ˆä¾‹å¦‚æ–‡æ¡£ä¸Šä¼ å¤±è´¥æ—¶æœªåˆ›å»ºï¼‰ï¼Œè¿™æ˜¯æ­£å¸¸æƒ…å†µ
            logger.info(f"ChromaDB é›†åˆ '{chroma_collection_name}' ä¸å­˜åœ¨ï¼Œæ— éœ€åˆ é™¤å‘é‡ã€‚")
        except Exception as e:
            # å…¶ä»–å¼‚å¸¸ï¼Œè®°å½•é”™è¯¯ä½†ä¸è®©ç¨‹åºå´©æºƒ
            logger.error(f"ä» ChromaDB åˆ é™¤æ–‡æ¡£å‘é‡å¤±è´¥: {e}", exc_info=True)

        # è§¦å‘BM25ç´¢å¼•çš„å…¨é‡é‡å»ºï¼Œä»¥ç¡®ä¿æ¸…ç†å¹²å‡€
        # æ•°æ®åº“çš„ON DELETE CASCADEä¼šè‡ªåŠ¨åˆ é™¤knowledge_fileså’Œfile_chunksä¸­çš„è®°å½•
        self._rebuild_bm25_index(kb_id)
        logger.info(f"å·²è§¦å‘çŸ¥è¯†åº“ID {kb_id} çš„BM25ç´¢å¼•å…¨é‡é‡å»ºã€‚")
        
        
    def _reciprocal_rank_fusion(self, search_results_list: List[List[Dict]], k=60) -> List[Dict]:
        """å¯¹å¤šè·¯æœç´¢ç»“æœè¿›è¡ŒRRFåˆå¹¶"""
        fused_scores = {}
        for results in search_results_list:
            for rank, result in enumerate(results):
                doc_content = result.get('content')
                if not isinstance(doc_content, str):
                    continue    # å¦‚æœå†…å®¹ä¸æ˜¯å­—ç¬¦ä¸²ï¼Œåˆ™è·³è¿‡æ­¤ç»“æœ
                if doc_content not in fused_scores:
                    fused_scores[doc_content] = {'score': 0, 'doc': result}
                fused_scores[doc_content]['score'] += 1 / (rank + k)
        
        reranked_results = sorted(fused_scores.values(), key=lambda x: x['score'], reverse=True)
        return [item['doc'] for item in reranked_results]

    def search(self, query: str, kb_ids: List[int]) -> List[Dict[str, Any]]:
        """åœ¨ä¸å½“å‰æ¨¡å‹åŒ¹é…çš„çŸ¥è¯†åº“ä¸­è¿›è¡Œè¯­ä¹‰æœç´¢ï¼Œæ‰§è¡Œæ··åˆæ£€ç´¢ã€HyDEã€Rerankï¼Œå¹¶è¿”å›ç”¨äºæº¯æºçš„ç»“æœã€‚"""
        if not kb_ids: return []
        
        top_k = config.get('knowledge_base.top_k', 10)
        rerank_top_n = config.get('knowledge_base.rerank_top_n', 3)
        relevance_threshold = config.get('knowledge_base.relevance_threshold', 1.2)
        
        final_query = query
        
        # å‡è®¾æ€§æ–‡æ¡£åµŒå…¥ï¼ˆHypothetical Document Embeddingsï¼Œç®€ç§° HyDEï¼‰æ˜¯ä¸€ç§ç”¨äºæå‡ä¿¡æ¯æ£€ç´¢ç³»ç»Ÿï¼Œç‰¹åˆ«æ˜¯æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰æµç¨‹æ•ˆæœçš„é«˜çº§æŠ€æœ¯ã€‚å…¶æ ¸å¿ƒæ€æƒ³éå¸¸å·§å¦™ï¼šä¸ç›´æ¥ä½¿ç”¨ç”¨æˆ·çš„åŸå§‹é—®é¢˜è¿›è¡Œæ£€ç´¢ï¼Œè€Œæ˜¯å…ˆè®©å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ ¹æ®è¯¥é—®é¢˜ç”Ÿæˆä¸€ä¸ªâ€œå‡è®¾æ€§â€çš„ç­”æ¡ˆæ–‡æ¡£ï¼Œç„¶ååˆ©ç”¨è¿™ä¸ªå‡è®¾æ–‡æ¡£å»æ£€ç´¢ä¸ä¹‹æœ€ç›¸å…³çš„çœŸå®æ–‡æ¡£ã€‚
        # --- 1. HyDE (å¦‚æœå¯ç”¨) ---
        if config.get('knowledge_base.enable_hyde', False):
            try:
                hyde_prompt = prompt_manager.render('hyde_generation.jinja2', user_query=query)
                hypothetical_answer, _ = llm_service.chat_completion([{"role": "user", "content": hyde_prompt}], topic_id=0, temperature=0.7)
                if hypothetical_answer:
                    final_query = cut_thinking_txt(hypothetical_answer)
                    logger.info(f"HyDE å·²å¯ç”¨ï¼Œä½¿ç”¨å‡è®¾æ€§æ–‡æ¡£è¿›è¡Œæœç´¢ã€‚")
            except Exception as e:
                logger.warning(f"HyDE ç”Ÿæˆå¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨åŸå§‹æŸ¥è¯¢ã€‚")
                
        # --- 2. æ··åˆæ£€ç´¢ ---
        all_search_results = []
        
        # 2.1 å‘é‡æ£€ç´¢ (Vector Search)
        try:
            active_embedding_model = llm_service.get_active_embedding_model_name()
            query_embedding = llm_service.get_embedding(final_query)
            if query_embedding:
                vector_results = []
                for kb_id in kb_ids:
                    collection_name = f"kb_{kb_id}_{active_embedding_model}"
                    try:
                        collection = self.client.get_collection(name=collection_name)
                        results = collection.query(query_embeddings=[query_embedding], n_results=top_k)
                        if results and results['ids'][0]:
                            for i in range(len(results['ids'][0])):
                                distance = results['distances'][0][i]
                                if distance < relevance_threshold:
                                    vector_results.append({
                                        "content": results['documents'][0][i],
                                        "metadata": results['metadatas'][0][i],
                                        "distance": distance
                                    })
                                else:
                                    # è®°å½•è¢«è¿‡æ»¤æ‰çš„æ–‡æ¡£ï¼Œç”¨äºè°ƒè¯•
                                    logger.debug(f"Vector search result filtered out by threshold. Distance: {distance:.4f} > {relevance_threshold}. Content: {results['documents'][0][i][:100]}...")
                    except Exception:
                        continue
                if vector_results:
                    all_search_results.append(vector_results)
        except Exception as e:
            logger.error(f"å‘é‡æœç´¢å¤±è´¥: {e}", exc_info=True)
            
        # 2.2 å…³é”®è¯æ£€ç´¢ (BM25 Search)
        bm25_results = []
        for kb_id in kb_ids:
            index_path = self._get_bm25_index_path(kb_id)
            if os.path.exists(index_path):
                with open(index_path, 'rb') as f:
                    data = pickle.load(f)
                    bm25 = data['bm25']
                    corpus = data['corpus']
                
                tokenized_query = query.split(" ")
                doc_scores = bm25.get_scores(tokenized_query)
                
                # è·å–åˆ†æ•°æœ€é«˜çš„ top_k ä¸ªç»“æœ
                top_indices = sorted(range(len(doc_scores)), key=lambda i: doc_scores[i], reverse=True)[:top_k]
                for i in top_indices:
                    score = doc_scores[i]
                    if score > 0:
                        bm25_results.append({"content": corpus[i], "metadata": {"source": "BM25 Keyword Search"}, "score": score})
                    else:
                        logger.debug(f"BM25 result filtered out by score <= 0. Score: {score:.4f}.")
        if bm25_results:
            all_search_results.append(bm25_results)

        if not all_search_results:
            logger.info("æ··åˆæ£€ç´¢åœ¨æ‰€æœ‰çŸ¥è¯†åº“ä¸­å‡æœªæ‰¾åˆ°ç›¸å…³ç»“æœã€‚")
            return []
        
        # --- 3. ç»“æœèåˆ (RRF) ---
        fused_results = self._reciprocal_rank_fusion(all_search_results)
        
        # --- 4. Reranker ç²¾æ’ ---
        documents_to_rerank = [res['content'] for res in fused_results]
        reranked_indices = llm_service.rerank(query, documents_to_rerank)
        final_results = [fused_results[i] for i in reranked_indices][:rerank_top_n]

        # --- 5. æ ¼å¼åŒ–è¾“å‡ºä»¥æ”¯æŒæº¯æº ---
        for i, res in enumerate(final_results):
            res['citation_id'] = f"æ¥æº-{i+1}"
        
        logger.info(f"æœç´¢å®Œæˆï¼Œæ··åˆæ£€ç´¢å…±æ‰¾åˆ° {len(fused_results)} æ¡ï¼ŒRerankåè¿”å›å‰ {len(final_results)} æ¡ã€‚")
        return final_results

# åˆ›å»ºä¸€ä¸ªå…¨å±€å®ä¾‹
kb_manager = KnowledgeBaseManager()
