# ğŸ“œ saga/utils/llm_service.py

import requests
import os
import io
import re
import json
from pathlib import Path
from datetime import datetime
from openai import OpenAI, APIError
from typing import List, Dict, Any, Union, Optional, Callable, Tuple, Generator

# --- å†…éƒ¨æ¨¡å—å¯¼å…¥ ---
from .config import config
from .logging_config import logger
from .database import db_manager
from .prompt_manager import prompt_manager

# --- ç¬¬ä¸‰æ–¹åº“å¯¼å…¥ ---
try:
    import tiktoken
    TOKENIZER = tiktoken.get_encoding("cl100k_base")
except ImportError:
    logger.error("tiktoken åº“æœªå®‰è£…ã€‚è¯·è¿è¡Œ 'pip install tiktoken'")
    TOKENIZER = None

try:
    import pdfplumber
except ImportError:
    pdfplumber = None

try:
    import PyPDF2
except ImportError:
    PyPDF2 = None
    
try:
    from PIL import Image
except ImportError:
    Image = None

try:
    import easyocr
    # é¦–æ¬¡è¿è¡Œæ—¶ä¼šè‡ªåŠ¨ä¸‹è½½æ¨¡å‹ï¼Œå¯èƒ½ä¼šæ¯”è¾ƒæ…¢
    EASYOCR_READER = easyocr.Reader(['ch_sim', 'en']) 
except ImportError:
    easyocr = None
    EASYOCR_READER = None
    
# MinerU é«˜è´¨é‡æ–‡æ¡£è§£ææœåŠ¡
try:
    from mineru.cli.common import do_parse
    from mineru.utils.enum_class import MakeMode
    MINERU_AVAILABLE = True
except ImportError:
    do_parse = None
    MakeMode = None
    MINERU_AVAILABLE = False
    logger.warning("MinerU æœªå®‰è£…ï¼Œå°†ä½¿ç”¨å¤‡ç”¨ OCR æœåŠ¡")

# Monkey patch è·³è¿‡æ¨¡å‹æ£€æŸ¥ï¼ˆå¦‚æœ MinerU å¯ç”¨ï¼‰
if MINERU_AVAILABLE:
    try:
        from mineru_vl_utils.vlm_client import http_client
        original_check = http_client.HttpVlmClient._check_model_name
        def noop_check(self, base_url, model_name):
            logger.debug(f"[MinerU] Skipping model check, using model: {model_name}")
        http_client.HttpVlmClient._check_model_name = noop_check
    except ImportError:
        logger.debug("MinerU VLM client not available, skipping monkey patch")

def count_tokens(text: str) -> int:
    """ä½¿ç”¨tiktokenè®¡ç®—æ–‡æœ¬çš„tokenæ•°é‡ã€‚å¦‚æœåº“ä¸å­˜åœ¨ï¼Œåˆ™è¿›è¡Œä¼°ç®—ã€‚"""
    if not text:
        return 0
    if TOKENIZER:
        return len(TOKENIZER.encode(text))
    else:
        # ä¼°ç®—ï¼šå¹³å‡ä¸€ä¸ªtokençº¦ç­‰äº4ä¸ªå­—ç¬¦
        return len(text) // 4
    
def cut_thinking_txt(text: str) -> str:
    """
    ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ç§»é™¤</think>å‰çš„å†…å®¹ï¼Œä¸“é—¨ç”¨äºå¤„ç†æ¨¡å‹çš„æ€è€ƒè¿‡ç¨‹ã€‚
    """
    if not text: return ""
    
    pattern = r'(.*?)<\/think>'
    result = re.sub(pattern, '', text, flags=re.DOTALL)
    result = re.sub(r'\n+', '\n', result).strip()
    return result

class LLMService:
    """
    LLMæœåŠ¡ç±»ï¼Œé‡‡ç”¨è‡ªåˆ·æ–°å•ä¾‹æ¨¡å¼ï¼Œèƒ½å¤ŸåŠ¨æ€å“åº”è¿è¡Œæ—¶çš„é…ç½®å˜æ›´ï¼Œå¹¶ç»Ÿä¸€è´Ÿè´£æ‰€æœ‰æ–‡ä»¶æ–‡æœ¬æå–ã€‚
    """
    _instance = None

    # ä½¿ç”¨ __new__ æ¥å®ç°å•ä¾‹æ¨¡å¼
    def __new__(cls, *args, **kwargs):
        if not cls._instance:
            cls._instance = super(LLMService, cls).__new__(cls, *args, **kwargs)
            # é¦–æ¬¡åˆ›å»ºæ—¶æ‰§è¡Œåˆå§‹åŒ–
            cls._instance._initialize_services()
        return cls._instance

    def _initialize_services(self):
        """å°è£…äº†å®Œæ•´çš„åˆå§‹åŒ–å’Œç»‘å®šé€»è¾‘ã€‚"""
        self.mode = config.active_llm_mode
        self.service_config = config.get_llm_config()
        self.conversation_config = config.get('conversation')
        
        self.chat_provider: Optional[Callable] = None
        self.embedding_provider: Optional[Callable] = None
        self.rerank_provider: Optional[Callable] = None
        
        self._init_clients()
        self._bind_providers()
        
        logger.info(f"LLMæœåŠ¡å·²åˆå§‹åŒ–/åˆ·æ–°ï¼Œå½“å‰ç”Ÿæ•ˆæ¨¡å¼: '{self.mode}'")

    def _check_and_refresh_config(self):
        """
        åœ¨æ¯æ¬¡å¤–éƒ¨è°ƒç”¨æ—¶æ£€æŸ¥é…ç½®ã€‚å¦‚æœæ¨¡å¼å·²æ›´æ”¹ï¼Œåˆ™å®Œå…¨é‡æ–°åˆå§‹åŒ–æœåŠ¡ã€‚
        """
        current_config_mode = config.active_llm_mode
        if self.mode != current_config_mode:
            logger.warning(f"æ£€æµ‹åˆ°æœåŠ¡æ¨¡å¼å·²ä» '{self.mode}' åˆ‡æ¢åˆ° '{current_config_mode}'ã€‚æ­£åœ¨é‡æ–°åŠ è½½æ‰€æœ‰æœåŠ¡...")
            # é‡æ–°æ‰§è¡Œå®Œæ•´çš„åˆå§‹åŒ–æµç¨‹
            self._initialize_services()
            logger.info("LLM æœåŠ¡å·²æˆåŠŸåˆ‡æ¢åˆ°æ–°æ¨¡å¼ã€‚")

    def _init_clients(self):
        """æ ¹æ®å½“å‰æ¨¡å¼åˆå§‹åŒ–æ‰€æœ‰å¯èƒ½çš„APIå®¢æˆ·ç«¯"""
        if self.mode == 'external':
            # æ–°é…ç½®ç»“æ„ï¼šæŒ‰æœåŠ¡ç±»å‹åˆ†ç»„ (chat/embedding/reranker/ocr)
            # æ¯ä¸ªæœåŠ¡ç±»å‹ä¸‹æœ‰å¤šä¸ªæä¾›å•†
            service_types = ['chat', 'embedding', 'reranker', 'ocr']

            for service_type in service_types:
                service_config = self.service_config.get(service_type, {})

                # è·å–æ‰€æœ‰æä¾›å•†é…ç½®
                for provider_name, provider_config in service_config.items():
                    if provider_name == 'active_provider':
                        continue

                    api_key = provider_config.get('api_key', '')
                    base_url = provider_config.get('base_url', '')

                    # è·³è¿‡æœªé…ç½®æˆ–å ä½ç¬¦APIå¯†é’¥
                    if not api_key or api_key.startswith('sk-your-') or api_key.startswith('your-'):
                        continue

                    # é¿å…é‡å¤åˆå§‹åŒ–åŒä¸€ä¸ªæä¾›å•†çš„å®¢æˆ·ç«¯
                    client_attr = f'{provider_name}_client'
                    if not hasattr(self, client_attr):
                        try:
                            client = OpenAI(api_key=api_key, base_url=base_url)
                            setattr(self, client_attr, client)
                            logger.info(f"{provider_name} å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ ({service_type})")
                        except Exception as e:
                            logger.error(f"åˆå§‹åŒ– {provider_name} å®¢æˆ·ç«¯å¤±è´¥ ({service_type}): {e}")
                            setattr(self, client_attr, None)

        # internal å’Œ local æ¨¡å¼çš„å®¢æˆ·ç«¯åœ¨å„è‡ªçš„æ–¹æ³•ä¸­æŒ‰éœ€åˆ›å»ºï¼Œæ— éœ€é¢„åˆå§‹åŒ–

    def _bind_providers(self):
        """ã€æ ¸å¿ƒã€‘æ ¹æ®å½“å‰æ¨¡å¼ï¼Œå°†å…·ä½“å®ç°ç»‘å®šåˆ°ç»Ÿä¸€çš„æœåŠ¡æ¥å£ä¸Šã€‚"""
        if self.mode == 'internal':
            self.chat_provider = self._internal_chat_completion
            self.embedding_provider = self._internal_get_embedding
            if self.service_config.get('reranker', {}).get('url'):
                self.rerank_provider = self._internal_rerank

        elif self.mode == 'external':
            # æ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•å¯ç”¨çš„å®¢æˆ·ç«¯
            has_valid_client = any(
                hasattr(self, f'{provider}_client') and getattr(self, f'{provider}_client') is not None
                for provider in ['qwen', 'deepseek', 'openai', 'anthropic', 'google', 'glm']
            )

            if not has_valid_client:
                # ä¸å†è‡ªåŠ¨é™çº§ï¼Œè€Œæ˜¯è®°å½•é”™è¯¯å¹¶è®¾ç½®æä¾›è€…ä¸ºNone
                error_msg = "å¤–éƒ¨APIæœªé…ç½®æˆ–æ— æ•ˆã€‚è¯·åœ¨ç³»ç»Ÿè®¾ç½®é¡µé¢é…ç½®è‡³å°‘ä¸€ä¸ªæä¾›å•†çš„APIå¯†é’¥ã€‚"
                logger.error(error_msg)
                # å°†æ‰€æœ‰æä¾›å•†è®¾ç½®ä¸ºNoneï¼Œä»¥ä¾¿åç»­è°ƒç”¨æ—¶æŠ›å‡ºæ˜ç¡®çš„é”™è¯¯
                self.chat_provider = None
                self.embedding_provider = None
                self.rerank_provider = None
                return

            self.chat_provider = self._external_chat_completion
            self.embedding_provider = self._external_get_embedding
            self.rerank_provider = self._external_rerank

        elif self.mode == 'local':
            self.chat_provider = self._local_chat_completion
            self.embedding_provider = self._local_get_embedding
            if self.service_config.get('reranker_model'):
                self.rerank_provider = self._local_rerank
            
    # --- å…¬å…±æ¥å£æ–¹æ³• (Public Interface Methods) ---
    # ã€é‡è¦ã€‘æ‰€æœ‰å…¬å…±æ–¹æ³•éƒ½å¿…é¡»åœ¨å¼€å¤´è°ƒç”¨ self._check_and_refresh_config()
    
    def extract_text_from_file(self, file_path: str) -> Optional[Dict[str, Any]]:
        """
        ç»Ÿä¸€çš„æ–‡ä»¶æ–‡æœ¬æå–æ¥å£ï¼Œæ ¹æ®å½“å‰æ¨¡å¼æ‰§è¡Œæ­£ç¡®çš„ç­–ç•¥ã€‚
        è¿”å›ä¸€ä¸ªåŒ…å« 'text' å’Œ 'doc_type' çš„å­—å…¸ï¼Œæˆ–åœ¨å¤±è´¥æ—¶è¿”å› Noneã€‚
        """
        self._check_and_refresh_config() # ç¡®ä¿ä½¿ç”¨æœ€æ–°çš„æ¨¡å¼

        file_ext = os.path.splitext(file_path)[1].lower()
        doc_type = "markdown" if file_ext == '.md' else "general"

        try:
            if self.mode == 'internal':
                # internal æ¨¡å¼ï¼šåªä½¿ç”¨å†…éƒ¨OCRï¼Œå¤±è´¥åˆ™å¤±è´¥ï¼Œä¸é™çº§
                if file_ext in ['.pdf', '.png', '.jpg', '.jpeg']:
                    logger.info(f"Internalæ¨¡å¼: æ­£åœ¨ä¸º '{file_path}' è°ƒç”¨å†…éƒ¨OCRæœåŠ¡...")
                    return self._internal_ocr(file_path)
                elif file_ext in ['.txt', '.md']:
                    logger.info(f"Internalæ¨¡å¼: æ­£åœ¨ä¸º '{file_path}' æ‰§è¡Œæœ¬åœ°æ–‡æœ¬æ–‡ä»¶è§£æ...")
                    return self._local_extraction(file_path, doc_type)
                else:
                    logger.warning(f"Internalæ¨¡å¼: ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹ '{file_ext}'ï¼Œæ— æ³•è§£æã€‚")
                    return None
            
            elif self.mode == 'external':
                # external æ¨¡å¼ï¼šåªä½¿ç”¨å¤–éƒ¨OCRï¼Œå¤±è´¥åˆ™å¤±è´¥ï¼Œä¸é™çº§
                logger.info(f"Externalæ¨¡å¼: æ­£åœ¨ä¸º '{file_path}' è°ƒç”¨å¤–éƒ¨OCRæœåŠ¡...")
            
                return self._external_ocr(file_path)

            elif self.mode == 'local':
                # local æ¨¡å¼ï¼šä¼˜å…ˆä½¿ç”¨ Ollama å¤šæ¨¡æ€ OCRï¼Œå¤±è´¥åˆ™ä½¿ç”¨æœ¬åœ°è§£æ
                if file_ext in ['.pdf', '.png', '.jpg', '.jpeg']:
                    logger.info(f"Localæ¨¡å¼: æ­£åœ¨ä¸º '{file_path}' è°ƒç”¨ Ollama OCR æœåŠ¡...")
                    result = self._local_ollama_ocr(file_path)
                    if result:
                        return result
                    # OCR å¤±è´¥ï¼Œé™çº§åˆ°æœ¬åœ°è§£æ
                    logger.info(f"Ollama OCR å¤±è´¥ï¼Œå°è¯•æœ¬åœ°è§£æ...")
                    return self._local_extraction(file_path, doc_type)
                elif file_ext in ['.txt', '.md']:
                    logger.info(f"Localæ¨¡å¼: æ­£åœ¨ä¸º '{file_path}' æ‰§è¡Œæœ¬åœ°æ–‡æœ¬æ–‡ä»¶è§£æ...")
                    return self._local_extraction(file_path, doc_type)
                else:
                    logger.warning(f"Localæ¨¡å¼: ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹ '{file_ext}'ï¼Œæ— æ³•è§£æã€‚")
                    return None

        except Exception as e:
            logger.error(f"åœ¨ '{self.mode}' æ¨¡å¼ä¸‹æå–æ–‡ä»¶ '{file_path}' æ–‡æœ¬å¤±è´¥: {e}", exc_info=True)
            return None

    # --- å„ç§æ¨¡å¼çš„ç§æœ‰å®ç° ---
    def _mineru_parse(self, file_path: str) -> Optional[Dict[str, Any]]:
        """
        ä½¿ç”¨ MinerU é«˜è´¨é‡æ–‡æ¡£è§£ææœåŠ¡è§£æ PDF æ–‡æ¡£ã€‚

        MinerU ä¼˜åŠ¿ï¼š
        - æ”¯æŒæ•°å­¦å…¬å¼è¯†åˆ«ï¼ˆLaTeX æ ¼å¼ï¼‰
        - æ”¯æŒå¤æ‚è¡¨æ ¼ç»“æ„
        - æ”¯æŒå¤šåˆ—å¸ƒå±€
        - æ”¯æŒå›¾ç‰‡å’Œå›¾è¡¨æè¿°
        - è¾“å‡ºç»“æ„åŒ– Markdown æ ¼å¼

        æ³¨æ„ï¼š
        - ä»…æ”¯æŒ PDF æ–‡ä»¶
        - ä»…åœ¨ä¼ä¸šå†…ç½‘ï¼ˆinternalï¼‰æ¨¡å¼ä¸‹å¯ç”¨
        - å¤§æ–‡æ¡£å¤„ç†æ—¶é—´è¾ƒé•¿ï¼Œè¯·è€å¿ƒç­‰å¾…
        - ç»“æœä¿å­˜åœ¨ output_dir ç›®å½•ä¸‹
        """
        # MinerU ä»…åœ¨ä¼ä¸šå†…ç½‘æ¨¡å¼ä¸‹å¯ç”¨
        if self.mode != 'internal':
            logger.debug(f"MinerU ä»…åœ¨ä¼ä¸šå†…ç½‘æ¨¡å¼ä¸‹å¯ç”¨ï¼Œå½“å‰æ¨¡å¼: {self.mode}")
            return None

        if not MINERU_AVAILABLE:
            logger.debug("MinerU ä¸å¯ç”¨ï¼ˆæœªå®‰è£… mineru åŒ…ï¼‰ï¼Œè·³è¿‡")
            return None

        mineru_config = self.service_config.get('mineru', {})
        if not mineru_config.get('enabled'):
            logger.debug("MinerU é…ç½®ä¸­æœªå¯ç”¨ï¼Œè·³è¿‡")
            return None

        file_ext = os.path.splitext(file_path)[1].lower()
        if file_ext != '.pdf':
            logger.debug(f"MinerU ä»…æ”¯æŒ PDF æ–‡ä»¶ï¼Œå½“å‰æ–‡ä»¶ç±»å‹: {file_ext}")
            return None

        server_url = mineru_config.get('server_url')
        model_name = mineru_config.get('model_name')
        output_dir = mineru_config.get('output_dir', 'data/mineru_output/')

        if not server_url or not model_name:
            logger.error("MinerU é…ç½®ä¸å®Œæ•´ï¼Œç¼ºå°‘ server_url æˆ– model_name")
            return None

        try:
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            Path(output_dir).mkdir(parents=True, exist_ok=True)

            # è®¾ç½®ç¯å¢ƒå˜é‡
            os.environ['MINERU_VL_MODEL_NAME'] = model_name

            # è¯»å– PDF æ–‡ä»¶
            with open(file_path, "rb") as f:
                pdf_bytes = f.read()
                file_size_mb = len(pdf_bytes) / (1024 * 1024)

            logger.info(f"ä½¿ç”¨ MinerU è§£æ PDF: {os.path.basename(file_path)} ({file_size_mb:.2f} MB)")
            logger.info("MinerU è§£æå¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…...")

            # è°ƒç”¨ MinerU è§£æ
            do_parse(
                output_dir=output_dir,
                pdf_file_names=[Path(file_path).name],
                pdf_bytes_list=[pdf_bytes],
                p_lang_list=["ch"],  # ä¸­æ–‡
                backend="vlm-http-client",
                parse_method="auto",
                formula_enable=True,   # å¯ç”¨å…¬å¼è¯†åˆ«
                table_enable=True,     # å¯ç”¨è¡¨æ ¼è¯†åˆ«
                server_url=server_url,
                f_dump_md=True,
                f_dump_middle_json=True,
                f_dump_content_list=True,
                f_make_md_mode=MakeMode.MM_MD
            )

            # è¯»å–è§£æç»“æœï¼ˆmarkdown æ ¼å¼ï¼‰
            # MinerU ä½¿ç”¨å®Œæ•´æ–‡ä»¶åï¼ˆå«.pdfæ‰©å±•åï¼‰åˆ›å»ºç›®å½•ï¼Œå¹¶åœ¨ vlm å­ç›®å½•ä¸­è¾“å‡º
            result_dir = Path(output_dir) / Path(file_path).name / "vlm"
            # MinerU è¾“å‡ºçš„ markdown æ–‡ä»¶åä¸º: åŸæ–‡ä»¶å.pdf.md
            md_file = result_dir / f"{Path(file_path).name}.md"

            if md_file.exists():
                with open(md_file, 'r', encoding='utf-8') as f:
                    extracted_text = f.read()

                logger.info(f"MinerU è§£ææˆåŠŸ: {os.path.basename(file_path)}ï¼Œæå–æ–‡æœ¬é•¿åº¦: {len(extracted_text)} å­—ç¬¦")
                return {
                    "text": extracted_text,
                    "doc_type": "mineru_processed",
                    "metadata": {
                        "parser": "MinerU",
                        "server_url": server_url,
                        "model": model_name,
                        "output_file": str(md_file)
                    }
                }
            else:
                logger.error(f"MinerU è§£æå®Œæˆï¼Œä½†æœªæ‰¾åˆ°è¾“å‡ºæ–‡ä»¶: {md_file}ï¼ŒæœŸæœ›ç›®å½•: {result_dir}")
                return None

        except Exception as e:
            logger.error(f"MinerU è§£æå¤±è´¥: {e}", exc_info=True)
            return None
            
    def _internal_ocr(self, file_path: str) -> Optional[Dict[str, Any]]:
        """
        è°ƒç”¨å†…éƒ¨OCRæœåŠ¡ï¼Œå¤„ç†PDFå’Œå›¾ç‰‡æ–‡ä»¶ã€‚

        å¤„ç†ç­–ç•¥ï¼š
        - PDF æ–‡ä»¶ï¼šä¼˜å…ˆä½¿ç”¨ MinerU é«˜è´¨é‡è§£æï¼ˆæ”¯æŒå…¬å¼ã€è¡¨æ ¼ã€å¤šåˆ—å¸ƒå±€ï¼‰
          å¤±è´¥æ—¶é™çº§åˆ°æ™®é€š OCR æœåŠ¡ï¼Œæ”¯æŒåˆ†å—å¤„ç†å¤§æ–‡æ¡£
        - å›¾ç‰‡æ–‡ä»¶ï¼ˆPNG/JPG/JPEGï¼‰ï¼šä½¿ç”¨æ™®é€š OCR æœåŠ¡
          æ”¯æŒè‡ªåŠ¨ç¼©æ”¾å¤„ç†é«˜æ¸…é•¿å›¾ç‰‡
        """
        file_ext = os.path.splitext(file_path)[1].lower()
        file_type = 'pdf' if file_ext == '.pdf' else 'image'

        # ========== PDF æ–‡ä»¶å¤„ç† ==========
        # ä¼˜å…ˆå°è¯•ä½¿ç”¨ MinerU é«˜è´¨é‡è§£æï¼ˆæ”¯æŒå…¬å¼ã€è¡¨æ ¼ã€å¤šåˆ—å¸ƒå±€ç­‰å¤æ‚å†…å®¹ï¼‰
        if file_ext == '.pdf':
            logger.info("æ£€æµ‹åˆ° PDF æ–‡ä»¶ï¼Œä¼˜å…ˆå°è¯•ä½¿ç”¨ MinerU è§£æ...")
            mineru_result = self._mineru_parse(file_path)
            if mineru_result:
                logger.info("MinerU è§£ææˆåŠŸï¼")
                return mineru_result
            else:
                logger.info("MinerU è§£æå¤±è´¥æˆ–ä¸å¯ç”¨ï¼Œé™çº§åˆ°æ™®é€š OCR æœåŠ¡...")

        # ========== é™çº§ï¼šä½¿ç”¨æ™®é€š OCR æœåŠ¡ ==========
        url = self.service_config.get('ocr', {}).get(f'{file_type}_url')

        if not url:
            logger.error(f"Internalæ¨¡å¼ä¸‹æœªé…ç½® '{file_type}' çš„OCR URLã€‚")
            return None

        with open(file_path, 'rb') as f:
            files = {'file': (os.path.basename(file_path), f)}
            try:
                response = requests.post(url, files=files, timeout=600)
                response.raise_for_status()
                api_result = response.json()

                # å…¼å®¹ä¸åŒOCRæœåŠ¡å¯èƒ½è¿”å›çš„æ ¼å¼
                if file_type == 'pdf':
                    text = api_result.get("result", {}).get("full_text", "")
                else: # image
                    text = api_result.get("result", {}).get("text", "")

                logger.info(f"å®Œæˆä» '{file_path}' æå–æœ‰æ•ˆæ–‡æœ¬")

                return {"text": text, "doc_type": "ocr_processed"}
            except requests.RequestException as e:
                logger.error(f"å†…éƒ¨OCRè¯·æ±‚å¤±è´¥: {e}")
                # å¤§æ–‡æ¡£é™çº§å¤„ç†
                if file_ext == '.pdf':
                    return self._process_large_pdf(file_path, url)
                # é«˜æ¸…é•¿å›¾ç‰‡é™çº§å¤„ç†
                elif file_ext in ['.png', '.jpg', '.jpeg']:
                    return self._handle_image_ocr(file_path, url)
                else:
                    return None
            
    def _process_large_pdf(self, file_path: str, url: str) -> Optional[Dict[str, Any]]:
        file_ext = os.path.splitext(file_path)[1].lower()
        if file_ext == '.pdf':
            file_basename = os.path.basename(file_path)
            logger.warning(f"PDF '{os.path.basename(file_path)}' æŒ‰ 5 é¡µ/å—çš„åˆ†å—å¤„ç†æ¨¡å¼ã€‚")
            all_texts = []
            try:
                with open(file_path, 'rb') as f:
                    pdf_reader = PyPDF2.PdfReader(f)
                    num_pages = len(pdf_reader.pages)
                    
                    # è¿­ä»£åˆ†å—
                    for i in range(0, num_pages, 5):
                        chunk_start = i
                        chunk_end = min(i + 5, num_pages)
                        
                        pdf_writer = PyPDF2.PdfWriter()
                        for page_num in range(chunk_start, chunk_end):
                            pdf_writer.add_page(pdf_reader.pages[page_num])
                        
                        page_buffer = io.BytesIO()
                        pdf_writer.write(page_buffer)
                        page_buffer.seek(0)

                        logger.info(f"æ­£åœ¨å‘é€PDFå— (é¡µç  {chunk_start + 1}-{chunk_end})...")
                        
                        chunk_filename = f"{os.path.splitext(file_basename)[0]}_pages_{chunk_start+1}-{chunk_end}.pdf"
                        files = {'file': (chunk_filename, page_buffer, 'application/pdf')}
                        
                        try:
                            # ä¸ºæ¯ä¸ªå—è®¾ç½®ä¸€ä¸ªè¾ƒçŸ­çš„è¶…æ—¶
                            chunk_response = requests.post(url, files=files, timeout=600)
                            chunk_response.raise_for_status()
                            api_result = chunk_response.json()
                            text = api_result.get("result", {}).get("full_text", "")
                            if text:
                                all_texts.append(text)
                            logger.info(f"PDFå— (é¡µç  {chunk_start + 1}-{chunk_end}) å¤„ç†æˆåŠŸã€‚")
                        except Exception as chunk_e:
                            logger.error(f"å¤„ç†PDFå— (é¡µç  {chunk_start + 1}-{chunk_end}) æ—¶å‘ç”Ÿé”™è¯¯: {chunk_e}")
                            continue # è·³è¿‡å¤±è´¥çš„å—

                final_text = "\n\n--- Chunk Break ---\n\n".join(all_texts)
                logger.info(f"PDF '{file_basename}' å·²é€šè¿‡åˆ†å—æ¨¡å¼å¤„ç†å®Œæˆã€‚")
                return {"text": final_text, "doc_type": "ocr_processed"}

            except Exception as fallback_e:
                logger.error(f"åœ¨åˆ†å—é™çº§å¤„ç†æ¨¡å¼ä¸‹å‘ç”Ÿä¸¥é‡é”™è¯¯: {fallback_e}", exc_info=True)
                return None
            
    def _handle_image_ocr(self, file_path: str, url: str) -> Optional[Dict[str, Any]]:
        """
        å¤„ç†é«˜æ¸…é•¿å›¾ç‰‡çš„ OCR è¯†åˆ«ã€‚

        ç­–ç•¥ï¼š
        1. æ£€æŸ¥å›¾ç‰‡å°ºå¯¸ï¼Œè¶…è¿‡ 4096px è‡ªåŠ¨ç­‰æ¯”ç¼©æ”¾
        2. è½¬æ¢ RGBA ä¸º RGB æ ¼å¼
        3. ä½¿ç”¨ JPEG æ ¼å¼ä¼ è¾“ä»¥å‡å°‘å¸¦å®½
        4. å¤±è´¥æ—¶è¿”å› None
        """
        image_url = url
        if not image_url: return None
        file_ext = os.path.splitext(file_path)[1].lower()
        if file_ext not in ['.png', '.jpg', '.jpeg']:
            return None

        file_basename = os.path.basename(file_path)
        MAX_DIMENSION = 4096  # OCR æœåŠ¡æ”¯æŒçš„æœ€å¤§å°ºå¯¸

        try:
            with Image.open(file_path) as img:
                original_size = img.size
                logger.info(f"å›¾ç‰‡å°ºå¯¸: {original_size[0]}x{original_size[1]}")

                # å¦‚æœå›¾ç‰‡è¶…è¿‡æœ€å¤§å°ºå¯¸ï¼Œç­‰æ¯”ç¼©æ”¾
                if img.size[0] > MAX_DIMENSION or img.size[1] > MAX_DIMENSION:
                    logger.info(f"å›¾ç‰‡å°ºå¯¸è¶…è¿‡ {MAX_DIMENSION}pxï¼Œæ‰§è¡Œç­‰æ¯”ç¼©æ”¾...")
                    img.thumbnail((MAX_DIMENSION, MAX_DIMENSION))
                    logger.info(f"ç¼©æ”¾åå°ºå¯¸: {img.size[0]}x{img.size[1]}")

                # è½¬æ¢æ ¼å¼ï¼šRGBA -> RGB
                img_buffer = io.BytesIO()
                if img.mode == 'RGBA' or 'transparency' in img.info:
                    img = img.convert('RGB')
                img.save(img_buffer, format='JPEG', quality=95)
                img_buffer.seek(0)

                logger.info(f"å‘é€å›¾ç‰‡åˆ° OCR æœåŠ¡ï¼Œç¼“å†²åŒºå¤§å°: {img_buffer.tell() / 1024:.1f} KB")

            files = {'file': (file_basename, img_buffer, 'image/jpeg')}
            response = requests.post(image_url, files=files, timeout=600)
            response.raise_for_status()
            text = response.json().get("result", {}).get("text", "")

            if text:
                logger.info(f"å›¾ç‰‡ OCR æˆåŠŸï¼Œæå–æ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦")
                return {"text": text, "doc_type": "ocr_processed"}
            else:
                logger.warning("å›¾ç‰‡ OCR è¿”å›ç©ºæ–‡æœ¬")
                return None

        except Exception as e:
            logger.error(f"å¤„ç†å›¾ç‰‡ '{file_basename}' æ—¶å‡ºé”™: {e}", exc_info=True)
            return None
            
    def _external_ocr(self, file_path: str) -> Optional[Dict[str, Any]]:
        """
        å¤–éƒ¨API OCRæ–‡æœ¬æå– - æ”¯æŒä¼˜å…ˆçº§é™çº§

        ä¼˜å…ˆä½¿ç”¨ slot_1ï¼Œå¤±è´¥åˆ™å°è¯• slot_2
        è®°å½• parse_source å’Œ parse_warning
        """
        # è·å–æ‰€æœ‰å¯ç”¨çš„OCRæ§½ä½ï¼ŒæŒ‰ä¼˜å…ˆçº§æ’åº
        enabled_slots = config.get_enabled_slots('ocr')

        if not enabled_slots:
            logger.warning("æœªé…ç½®å¯ç”¨çš„OCRæ§½ä½")
            return None

        # æŒ‰ä¼˜å…ˆçº§å°è¯•æ¯ä¸ªæ§½ä½
        for slot_num in enabled_slots:
            slot_config = config.get_slot_config('ocr', slot_num)
            provider = slot_config.get('provider', '')
            model = slot_config.get('model_name', '')
            base_url = slot_config.get('base_url', '')
            api_key = slot_config.get('api_key', '')

            if not provider or not model or not base_url:
                logger.warning(f"æ§½ä½ {slot_num} é…ç½®ä¸å®Œæ•´ï¼Œè·³è¿‡")
                continue

            logger.info(f"å°è¯•ä½¿ç”¨æ§½ä½ {slot_num} ({provider}/{model}) å¤„ç†æ–‡ä»¶: {file_path}")

            result = self._ocr_with_slot(file_path, provider, model, base_url, api_key, slot_num)

            if result:
                # æˆåŠŸè§£æ
                result['parse_source'] = f'slot_{slot_num}'
                if slot_num != enabled_slots[0]:
                    # å¦‚æœä½¿ç”¨çš„æ˜¯å¤‡ç”¨æ§½ä½ï¼Œæ·»åŠ è­¦å‘Š
                    result['parse_warning'] = f"ä¸»æ§½ä½ (slot_{enabled_slots[0]}) è§£æå¤±è´¥ï¼Œå·²è‡ªåŠ¨åˆ‡æ¢åˆ°å¤‡ç”¨æ§½ä½ (slot_{slot_num})"
                return result
            else:
                logger.warning(f"æ§½ä½ {slot_num} è§£æå¤±è´¥ï¼Œå°è¯•ä¸‹ä¸€ä¸ªæ§½ä½")

        # æ‰€æœ‰æ§½ä½éƒ½å¤±è´¥
        logger.error("æ‰€æœ‰OCRæ§½ä½å‡è§£æå¤±è´¥")
        return None

    def _ocr_with_slot(self, file_path: str, provider: str, model: str, base_url: str, api_key: str, slot_num: int) -> Optional[Dict[str, Any]]:
        """
        ä½¿ç”¨æŒ‡å®šæ§½ä½è¿›è¡ŒOCRè§£æ

        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            provider: æä¾›å•†åç§°
            model: æ¨¡å‹åç§°
            base_url: Base URL
            api_key: APIå¯†é’¥
            slot_num: æ§½ä½ç¼–å·

        Returns:
            è§£æç»“æœå­—å…¸ï¼Œå¤±è´¥è¿”å›None
        """
        try:
            if provider == 'qwen':
                return self._qwen_ocr(file_path, model, api_key)
            elif provider in ['deepseek', 'openai', 'anthropic', 'google', 'glm']:
                return self._vision_ocr(file_path, provider, model, api_key)
            else:
                logger.warning(f"æä¾›å•† {provider} æš‚ä¸æ”¯æŒOCR")
                return None

        except Exception as e:
            logger.error(f"æ§½ä½ {slot_num} ({provider}/{model}) OCRå¤±è´¥: {e}")
            return None

    def _qwen_ocr(self, file_path: str, model: str, api_key: str) -> Optional[Dict[str, Any]]:
        """ä½¿ç”¨é€šä¹‰åƒé—®è¿›è¡ŒOCRï¼ˆæ”¯æŒé•¿æ–‡æ¡£ï¼‰"""
        try:
            # åˆ›å»ºä¸´æ—¶å®¢æˆ·ç«¯
            client = OpenAI(api_key=api_key, base_url="https://dashscope.aliyuncs.com/compatible-mode/v1")

            from pathlib import Path
            file_obj = client.files.create(
                file=Path(file_path),
                purpose="file-extract"
            )

            logger.info(f"æ–‡ä»¶ä¸Šä¼ æˆåŠŸï¼Œæ–‡ä»¶ID: {file_obj.id}")

            parser_prompt = prompt_manager.render('doc_parser_prompt.jinja2')
            messages = [
                {'role': 'system', 'content': parser_prompt},
                {'role': 'system', 'content': f'fileid://{file_obj.id}'},
                {'role': 'user', 'content': 'è¯·æå–æ–‡æ¡£ä¸­çš„æ‰€æœ‰å†…å®¹ï¼Œç²¾å‡†çš„ä¸­æ–‡è¾“å‡º'}
            ]

            completion = client.chat.completions.create(
                model=model,
                messages=messages,
                temperature=0.1,
                max_tokens=32768,
                stream=False
            )

            extracted_text = completion.choices[0].message.content

            if hasattr(completion, 'usage'):
                usage = completion.usage
                logger.info(f"OCRæå–æ¶ˆè€—token: è¾“å…¥={usage.prompt_tokens}, è¾“å‡º={usage.completion_tokens}, æ€»è®¡={usage.total_tokens}")

            if extracted_text:
                return {
                    "text": extracted_text,
                    "doc_type": "ocr_processed",
                    "metadata": {
                        "model": model,
                        "file_id": file_obj.id,
                        "file_type": os.path.splitext(file_path)[1],
                        "token_usage": completion.usage.model_dump() if hasattr(completion, 'usage') else None
                    }
                }
            else:
                logger.warning("OCRæå–è¿”å›ç©ºæ–‡æœ¬")
                return None

        except Exception as e:
            logger.error(f"é€šä¹‰åƒé—®OCRå¤±è´¥: {e}")
            return None

    def _vision_ocr(self, file_path: str, provider: str, model: str, api_key: str) -> Optional[Dict[str, Any]]:
        """ä½¿ç”¨è§†è§‰æ¨¡å‹è¿›è¡ŒOCRï¼ˆé€‚ç”¨äºå›¾ç‰‡ï¼‰"""
        try:
            # æ£€æŸ¥æ–‡ä»¶ç±»å‹
            file_ext = os.path.splitext(file_path)[1].lower()
            if file_ext not in ['.png', '.jpg', '.jpeg', '.gif', '.bmp', '.webp']:
                logger.warning(f"è§†è§‰æ¨¡å‹OCRä»…æ”¯æŒå›¾ç‰‡æ–‡ä»¶ï¼Œå½“å‰æ–‡ä»¶: {file_ext}")
                return None

            # è¯»å–å›¾ç‰‡å¹¶è½¬æ¢ä¸ºbase64
            import base64
            with open(file_path, 'rb') as f:
                image_data = base64.b64encode(f.read()).decode('utf-8')

            # æ„å»ºè¯·æ±‚
            client = OpenAI(api_key=api_key)

            messages = [
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": "è¯·æå–å›¾ç‰‡ä¸­çš„æ‰€æœ‰æ–‡å­—å†…å®¹ï¼Œä¿æŒåŸæœ‰æ ¼å¼å’Œç»“æ„ã€‚"},
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/jpeg;base64,{image_data}"
                            }
                        }
                    ]
                }
            ]

            completion = client.chat.completions.create(
                model=model,
                messages=messages,
                temperature=0.1,
                max_tokens=4096
            )

            extracted_text = completion.choices[0].message.content

            if extracted_text:
                return {
                    "text": extracted_text,
                    "doc_type": "ocr_image",
                    "metadata": {
                        "model": model,
                        "file_type": file_ext,
                        "token_usage": completion.usage.model_dump() if hasattr(completion, 'usage') else None
                    }
                }
            else:
                logger.warning("è§†è§‰æ¨¡å‹OCRè¿”å›ç©ºæ–‡æœ¬")
                return None

        except Exception as e:
            logger.error(f"{provider} è§†è§‰æ¨¡å‹OCRå¤±è´¥: {e}")
            return None

    def _local_ollama_ocr(self, file_path: str) -> Optional[Dict[str, Any]]:
        """
        ä½¿ç”¨æœ¬åœ° Ollama å¤šæ¨¡æ€æ¨¡å‹è¿›è¡Œ OCRã€‚

        æ”¯æŒ PDF å’Œå›¾ç‰‡æ–‡ä»¶ï¼Œä½¿ç”¨é…ç½®çš„å¤šæ¨¡æ€æ¨¡å‹ï¼ˆå¦‚ qwen3-vl:2bï¼‰ã€‚
        """
        local_config = self.service_config
        ocr_model = local_config.get('ocr_model', '')
        host = local_config.get('host', 'http://localhost:11434')

        if not ocr_model:
            logger.warning("æœ¬åœ° Ollama æœªé…ç½® OCR æ¨¡å‹")
            return None

        file_ext = os.path.splitext(file_path)[1].lower()

        try:
            # å¯¹äº PDFï¼Œéœ€è¦å…ˆè½¬æ¢ä¸ºå›¾ç‰‡
            if file_ext == '.pdf':
                # æ£€æŸ¥æ˜¯å¦å®‰è£…äº† pdf2image
                try:
                    from pdf2image import convert_from_path
                    # å°† PDF ç¬¬ä¸€é¡µè½¬æ¢ä¸ºå›¾ç‰‡
                    images = convert_from_path(file_path, first_page=1, last_page=1)
                    if not images:
                        logger.error(f"PDF è½¬æ¢å›¾ç‰‡å¤±è´¥: {file_path}")
                        return None
                    # ä¿å­˜ä¸´æ—¶å›¾ç‰‡
                    import tempfile
                    with tempfile.NamedTemporaryFile(suffix='.png', delete=False) as tmp:
                        images[0].save(tmp.name, 'PNG')
                        image_path = tmp.name
                except ImportError:
                    logger.warning("æœªå®‰è£… pdf2imageï¼Œæ— æ³•å¤„ç† PDF æ–‡ä»¶")
                    return None
                except Exception as e:
                    logger.error(f"PDF è½¬å›¾ç‰‡å¤±è´¥: {e}")
                    return None
            else:
                image_path = file_path

            # è¯»å–å›¾ç‰‡å¹¶ç¼–ç ä¸º base64
            import base64
            with open(image_path, 'rb') as f:
                image_data = base64.b64encode(f.read()).decode('utf-8')

            # æ„å»º Ollama API è¯·æ±‚
            import requests
            api_url = f"{host}/api/chat"

            payload = {
                "model": ocr_model,
                "stream": False,
                "messages": [
                    {
                        "role": "user",
                        "content": "è¯·è¯†åˆ«è¿™å¼ å›¾ç‰‡ä¸­çš„æ‰€æœ‰æ–‡å­—å†…å®¹ï¼ŒåŒ…æ‹¬è¡¨æ ¼ã€å…¬å¼ç­‰ã€‚è¯·å®Œæ•´åœ°è¾“å‡ºè¯†åˆ«åˆ°çš„æ–‡å­—ï¼Œä¿æŒåŸæœ‰çš„æ ¼å¼å’Œç»“æ„ã€‚"
                    }
                ],
                "images": [image_data]
            }

            response = requests.post(api_url, json=payload, timeout=120)
            response.raise_for_status()
            result = response.json()

            if result and 'message' in result and 'content' in result['message']:
                text = result['message']['content'].strip()
                logger.info(f"Ollama OCR æˆåŠŸè¯†åˆ«æ–‡æœ¬ï¼Œé•¿åº¦: {len(text)} å­—ç¬¦")

                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                if file_ext == '.pdf' and image_path != file_path:
                    try:
                        os.unlink(image_path)
                    except:
                        pass

                return {
                    "text": text,
                    "doc_type": "ocr_processed",
                    "metadata": {
                        "model": ocr_model,
                        "file_type": file_ext,
                        "source": "local_ollama"
                    }
                }
            else:
                logger.warning("Ollama OCR è¿”å›ç©ºæ–‡æœ¬")
                return None

        except Exception as e:
            logger.error(f"Ollama OCR å¤±è´¥: {e}")
            return None

    def _local_extraction(self, file_path: str, doc_type: str) -> Optional[Dict[str, Any]]:
        """
        æœ¬åœ°æ–‡ä»¶è§£æçš„é›†åˆã€‚å®ƒä¼šå°è¯•å¤šç§åº“æ¥æå–æ–‡æœ¬ã€‚
        """
        file_ext = os.path.splitext(file_path)[1].lower()
        text = ""

        if file_ext == '.pdf':
            # ç­–ç•¥1ï¼šä¼˜å…ˆä½¿ç”¨ pdfplumber
            if pdfplumber:
                try:
                    with pdfplumber.open(file_path) as pdf:
                        text = "\n".join(page.extract_text() for page in pdf.pages if page.extract_text())
                    logger.info(f"ä½¿ç”¨ pdfplumber æˆåŠŸæå–PDF: {os.path.basename(file_path)}")
                except Exception as e:
                    logger.warning(f"pdfplumber æå–å¤±è´¥ ({e})ï¼Œå°è¯•å…¶ä»–æ–¹æ³•...")
                    text = ""

            # ç­–ç•¥2ï¼šå¦‚æœ pdfplumber å¤±è´¥æˆ–æœªå®‰è£…ï¼Œæˆ–æå–æ–‡æœ¬è¿‡å°‘ï¼Œä½¿ç”¨ PyPDF2
            if not text and PyPDF2:
                try:
                    with open(file_path, 'rb') as f:
                        reader = PyPDF2.PdfReader(f)
                        text = "\n".join(page.extract_text() for page in reader.pages if page.extract_text())
                    logger.info(f"ä½¿ç”¨ PyPDF2 æˆåŠŸæå–PDF: {os.path.basename(file_path)}")
                except Exception as e:
                    logger.warning(f"PyPDF2 æå–å¤±è´¥ ({e})...")
                    text = ""
        
        elif file_ext in ['.png', '.jpg', '.jpeg']:
            if EASYOCR_READER:
                try:
                    result = EASYOCR_READER.readtext(file_path, detail=0, paragraph=True)
                    text = "\n".join(result)
                    logger.info(f"ä½¿ç”¨ easyocr æˆåŠŸè¯†åˆ«å›¾ç‰‡: {os.path.basename(file_path)}")
                except Exception as e:
                    logger.error(f"easyocr è¯†åˆ«å¤±è´¥: {e}")
            else:
                logger.warning("easyocr æœªå®‰è£…ï¼Œæ— æ³•è¯†åˆ«å›¾ç‰‡ã€‚")

        elif file_ext in ['.txt', '.md']:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                text = f.read()
        
        
        if text:
            logger.info(f"å®Œæˆä» '{file_path}' æå–æœ‰æ•ˆæ–‡æœ¬")
            
            return {"text": text, "doc_type": doc_type}
        else:
            logger.error(f"æ‰€æœ‰æœ¬åœ°æ–¹æ³•å‡æœªèƒ½ä» '{file_path}' æå–å‡ºæœ‰æ•ˆæ–‡æœ¬ã€‚")
            return None

    def get_active_embedding_model_name(self) -> str:
        """
        è·å–å½“å‰æ¿€æ´»çš„ embedding æ¨¡å‹çš„å®‰å…¨åç§°ï¼Œç”¨äºæ„å»º collection åç§°ã€‚

        æ”¯æŒæ–°çš„ slot-based é…ç½®ï¼š
        - è·å–æ¿€æ´»çš„ embedding æ§½ä½
        - ä½¿ç”¨æ§½ä½çš„ provider å’Œ model_name æ„å»ºå”¯ä¸€æ ‡è¯†
        """
        self._check_and_refresh_config()

        if self.mode == 'internal':
            model = self.service_config.get('embedding', {}).get('model', 'default_internal')
        elif self.mode == 'local':
            model = self.service_config.get('embedding_model', 'default_local')
        elif self.mode == 'external':
            # ä½¿ç”¨æ–°çš„ slot-based é…ç½®è·å–æ¿€æ´»çš„ embedding æ§½ä½
            active_slot = config.get_active_embedding_slot()
            if active_slot:
                slot_config = config.get_slot_config('embedding', active_slot)
                provider = slot_config.get('provider', '')
                model_name = slot_config.get('model_name', 'default_external')
                # æ·»åŠ æä¾›å•†å‰ç¼€ä»¥ç¡®ä¿ä¸åŒæä¾›å•†çš„æ¨¡å‹æœ‰å”¯ä¸€åç§°
                model = f"{provider}_{model_name}" if provider else model_name
            else:
                # å¦‚æœæ²¡æœ‰æ¿€æ´»çš„æ§½ä½ï¼Œå°è¯•è·å–ç¬¬ä¸€ä¸ªé…ç½®çš„æ§½ä½
                enabled_slots = config.get_enabled_slots('embedding')
                if enabled_slots:
                    slot_config = config.get_slot_config('embedding', enabled_slots[0])
                    provider = slot_config.get('provider', '')
                    model_name = slot_config.get('model_name', 'default_external')
                    model = f"{provider}_{model_name}" if provider else model_name
                else:
                    model = 'default_external'
        else:
            model = 'default'
        # æ¸…ç†ç‰¹æ®Šå­—ç¬¦
        return re.sub(r'[^a-zA-Z0-9_.-]', '_', model)

    def get_active_chat_slot_info(self) -> Dict[str, Any]:
        """
        è·å–å½“å‰æ¿€æ´»çš„èŠå¤©æ§½ä½ä¿¡æ¯

        Returns:
            åŒ…å« slot_num, provider, model_name, display_name çš„å­—å…¸
        """
        self._check_and_refresh_config()

        if self.mode == 'external':
            enabled_slots = config.get_enabled_slots('chat')
            if enabled_slots:
                slot_num = enabled_slots[0]  # è·å–ä¼˜å…ˆçº§æœ€é«˜çš„æ§½ä½
                slot_config = config.get_slot_config('chat', slot_num)
                return {
                    'slot_num': slot_num,
                    'provider': slot_config.get('provider', ''),
                    'model_name': slot_config.get('model_name', ''),
                    'display_name': slot_config.get('display_name', '')
                }
        return {}

    def get_all_enabled_chat_slots(self) -> List[Dict[str, Any]]:
        """
        è·å–æ‰€æœ‰å¯ç”¨çš„èŠå¤©æ§½ä½ä¿¡æ¯ï¼ˆç”¨äºåŒæ¨¡å‹å¯¹æ¯”ï¼‰

        Returns:
            åŒ…å«æ‰€æœ‰å¯ç”¨æ§½ä½ä¿¡æ¯çš„åˆ—è¡¨
        """
        self._check_and_refresh_config()

        if self.mode == 'external':
            enabled_slots = config.get_enabled_slots('chat')
            result = []
            for slot_num in enabled_slots:
                slot_config = config.get_slot_config('chat', slot_num)
                result.append({
                    'slot_num': slot_num,
                    'provider': slot_config.get('provider', ''),
                    'model_name': slot_config.get('model_name', ''),
                    'display_name': slot_config.get('display_name', '')
                })
            return result
        return []

    # --- ç»Ÿä¸€çš„å…¬å…±æ¥å£ ---
    
    def chat_completion_stream(self, messages: List[Dict[str, str]], topic_id: int, **kwargs) -> Generator[str, None, None]:
        """æ‰§è¡Œæµå¼èŠå¤©è¡¥å…¨ï¼Œå†…ç½®ä¸Šä¸‹æ–‡ç®¡ç†ã€‚"""
        self._check_and_refresh_config()

        if not self.chat_provider:
            if self.mode == 'external':
                raise RuntimeError("å¤–éƒ¨APIæœªé…ç½®ã€‚è¯·åœ¨âš™ï¸ç³»ç»Ÿè®¾ç½®é¡µé¢é…ç½®è‡³å°‘ä¸€ä¸ªæä¾›å•†çš„APIå¯†é’¥ã€‚")
            else:
                raise RuntimeError(f"æ¨¡å¼ '{self.mode}' ä¸‹æ²¡æœ‰å¯ç”¨çš„èŠå¤©æœåŠ¡ã€‚")

        # ä¸Šä¸‹æ–‡ç®¡ç†é€»è¾‘ä¿æŒä¸å˜
        processed_messages, _ = self._manage_context_length(messages, topic_id)

        # ç¡®ä¿kwargsä¸­åŒ…å« stream=True
        kwargs['stream'] = True

        # --- è·å–æµå¼å“åº” ---
        if self.mode == 'internal':
            # å†…éƒ¨æœåŠ¡æ¨¡å¼
            client = OpenAI(base_url=self.service_config['llm']['url'], api_key=self.service_config['llm']['api_key'])
            model = self.service_config['llm']['model']

            try:
                stream = client.chat.completions.create(model=model, messages=processed_messages, **kwargs)
                for chunk in stream:
                    content = chunk.choices[0].delta.content
                    if content:
                        yield content
            finally:
                client.close()

        elif self.mode == 'external':
            # å¤–éƒ¨APIæ¨¡å¼ - ä½¿ç”¨é€‰ä¸­çš„èŠå¤©æ¨¡å‹
            selected = config.get_selected_model('chat')
            provider = selected.get('provider')
            model = selected.get('model')

            if not provider or not model:
                raise RuntimeError("æœªé…ç½®é€‰ä¸­çš„èŠå¤©æ¨¡å‹")

            client = getattr(self, f'{provider}_client', None)
            if not client:
                raise RuntimeError(f"æœªæ‰¾åˆ° {provider} å®¢æˆ·ç«¯")

            stream = client.chat.completions.create(model=model, messages=processed_messages, **kwargs)
            for chunk in stream:
                content = chunk.choices[0].delta.content
                if content:
                    yield content

        elif self.mode == 'local':
            # Ollama
            url = f"{self.service_config['host']}/api/chat"
            payload = {"model": self.service_config['chat_model'], "messages": processed_messages, "stream": True, "options": kwargs}
            with requests.post(url, json=payload, stream=True) as response:
                response.raise_for_status()
                for line in response.iter_lines():
                    if line:
                        chunk = json.loads(line)
                        content = chunk.get('message', {}).get('content')
                        if content:
                            yield content
    
    def chat_completion(self, messages: List[Dict[str, str]], topic_id: int, **kwargs) -> Optional[str]:
        """
        ä¸»æ€è€ƒå›è·¯ - é‡é‡çº§å‡½æ•°
        ç”¨äºå¤„ç†å®Œæ•´çš„ç”¨æˆ·å¯¹è¯ï¼Œå†…ç½®å¤æ‚çš„ä¸Šä¸‹æ–‡ç®¡ç†å’Œæ‘˜è¦é€»è¾‘ã€‚
        """
        self._check_and_refresh_config()
        
        if not self.chat_provider:
            raise RuntimeError(f"æ¨¡å¼ '{self.mode}' ä¸‹æ²¡æœ‰å¯ç”¨çš„èŠå¤©æœåŠ¡ã€‚")
        
        processed_messages, stats = self._manage_context_length(messages, topic_id)
        
        response = self.chat_provider(processed_messages, **kwargs)
        
        # å°†æœ¬æ¬¡è¯·æ±‚/å“åº”çš„tokenç»Ÿè®¡ä¿¡æ¯è¿”å›
        # å¯ä»¥åœ¨UIå±‚ä½¿ç”¨
        if response:
            stats['response_tokens'] = count_tokens(response)
            stats['total_tokens'] = stats['request_tokens'] + stats['response_tokens']
        
        return response, stats
    
    def _lightweight_chat_completion(self, messages: List[Dict[str, str]], **kwargs) -> Optional[str]:
        """
        è¾…åŠ©æ€è€ƒå›è·¯ - è½»é‡çº§å‡½æ•°
        ç”¨äºå†…éƒ¨ã€ä¸€æ¬¡æ€§çš„ä»»åŠ¡ï¼Œå¦‚ç”Ÿæˆæ ‡é¢˜ã€ç”Ÿæˆæ‘˜è¦ã€‚
        å®ƒç›´æ¥è°ƒç”¨ chat_providerï¼Œå®Œå…¨ç»•è¿‡å¤æ‚çš„ä¸Šä¸‹æ–‡ç®¡ç†ã€‚
        """
        self._check_and_refresh_config()
        if not self.chat_provider:
            raise RuntimeError(f"æ¨¡å¼ '{self.mode}' ä¸‹æ²¡æœ‰å¯ç”¨çš„èŠå¤©æœåŠ¡ã€‚")
        
        # ç¡®ä¿éæµå¼
        # kwargs['stream'] = False
        
        # ç›´æ¥è°ƒç”¨ï¼Œä¸è¿›è¡Œä»»ä½•å¤„ç†
        response = self.chat_provider(messages, **kwargs)
        return response

    def get_embedding(self, content: Union[str, List[str]], **kwargs) -> Optional[Union[List[float], List[List[float]]]]:
        """æ‰§è¡Œæ–‡æœ¬å‘é‡åŒ–ã€‚"""
        self._check_and_refresh_config()
        
        if not self.embedding_provider:
            raise RuntimeError(f"æ¨¡å¼ '{self.mode}' ä¸‹æ²¡æœ‰å¯ç”¨çš„EmbeddingæœåŠ¡ã€‚")
        return self.embedding_provider(content, **kwargs)

    def rerank(self, query: str, documents: List[str], **kwargs) -> Optional[List[int]]:
        """å¯¹æ–‡æ¡£åˆ—è¡¨æ ¹æ®æŸ¥è¯¢è¿›è¡Œé‡æ’åºï¼Œè¿”å›æ’åºåçš„ç´¢å¼•ã€‚"""
        self._check_and_refresh_config()
        if not self.rerank_provider:
            logger.warning("å½“å‰æ¨¡å¼ä¸‹æ²¡æœ‰å¯ç”¨çš„RerankeræœåŠ¡ï¼Œå°†è·³è¿‡é‡æ’åºã€‚")
            return list(range(len(documents))) # è¿”å›åŸå§‹é¡ºåº
        return self.rerank_provider(query, documents, **kwargs)

    # --- å†…éƒ¨ç§æœ‰æ–¹æ³• (Internal Private Methods) ---
    # --- ä¸Šä¸‹æ–‡ç®¡ç† ---
    def _manage_context_length(self, messages: List[Dict[str, str]], topic_id: int) -> Tuple[List[Dict[str, str]], Dict]:
        """
        æ­¤ç‰ˆæœ¬é‡æ„äº†æ ¸å¿ƒç®—æ³•ï¼Œä¿è¯ system prompt å’Œæœ€æ–°çš„ user prompt æ°¸è¿œä¸ä¼šè¢«ä¸¢å¼ƒã€‚
        """
        budget = self.conversation_config.get('context_token_budget', 6000)
        
        # 1. åˆå§‹åˆ†è§£
        system_msgs = [m for m in messages if m['role'] == 'system']
        dialog_msgs = [m for m in messages if m['role'] in ['user', 'assistant']]
        
        if not dialog_msgs: # å¦‚æœæ²¡æœ‰ä»»ä½•å¯¹è¯ï¼ˆä¾‹å¦‚ï¼Œåªç”¨äºæ‘˜è¦çš„è°ƒç”¨ï¼‰ï¼Œåˆ™ç›´æ¥è¿”å›
            return system_msgs, {}

        # 2. ç»å¯¹ä¿æŠ¤æœ€æ–°ä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
        latest_user_message = dialog_msgs.pop()
        past_history = dialog_msgs # dialog_msgs ç°åœ¨åªåŒ…å«â€œè¿‡å»â€çš„å†å²

        # 3. è®¡ç®—â€œä¸å¯å˜â€éƒ¨åˆ†ï¼ˆç³»ç»Ÿ+æœ€æ–°é—®é¢˜ï¼‰çš„Tokenï¼Œå¹¶è®¡ç®—â€œå†å²â€çš„å¯ç”¨é¢„ç®—
        system_tokens = sum(count_tokens(m['content']) for m in system_msgs)
        latest_user_tokens = count_tokens(latest_user_message['content'])
        
        history_budget = budget - system_tokens - latest_user_tokens

        # 4. ä»åå¾€å‰ï¼ˆä»æ–°åˆ°æ—§ï¼‰å¡«å……â€œè¿‡å»çš„å†å²â€
        final_history_msgs = []
        if history_budget > 0:
            temp_history_tokens = 0
            for msg in reversed(past_history):
                msg_tokens = count_tokens(msg['content'])
                if temp_history_tokens + msg_tokens > history_budget:
                    break
                final_history_msgs.insert(0, msg)
                temp_history_tokens += msg_tokens
        
        # 5. åˆ¤æ–­æ˜¯å¦éœ€è¦ã€æ»šåŠ¨æ›´æ–°ã€‘æ‘˜è¦
        num_kept_history = len(final_history_msgs)
        num_original_history = len(past_history)

        if num_kept_history < num_original_history:
            # åªæœ‰å½“â€œè¿‡å»çš„å†å²â€è¢«æˆªæ–­æ—¶ï¼Œæ‰è§¦å‘æ‘˜è¦
            cutoff_index = num_original_history - num_kept_history
            msgs_to_summarize_content = [f"{m['role']}: {m['content']}" for m in past_history[:cutoff_index]]
            
            existing_summary = db_manager.get_topic_summary(topic_id)
            
            text_for_new_summary = "\n".join(msgs_to_summarize_content)
            if existing_summary:
                text_for_new_summary = existing_summary + "\n\n" + text_for_new_summary
            
            self._generate_and_save_summary(text_for_new_summary, topic_id)

        # 6. æœ€ç»ˆç»„è£…
        #    å†æ¬¡è·å–æœ€æ–°çš„æ‘˜è¦ï¼ˆå¯èƒ½æ˜¯åˆšåˆšç”Ÿæˆçš„ï¼‰
        final_summary = db_manager.get_topic_summary(topic_id)
        if final_summary:
            summary_str = f"\n\nã€å‰æƒ…æè¦ã€‘\n{final_summary}"
            # åˆå¹¶åˆ° system prompt ä¸­
            if system_msgs and summary_str not in system_msgs[0]['content']:
                system_msgs[0]['content'] += summary_str

        # æœ€ç»ˆæ¶ˆæ¯ = ç³»ç»Ÿ(å«æ‘˜è¦) + å¡«å……çš„å†å² + æœ€æ–°çš„é—®é¢˜
        final_messages = system_msgs + final_history_msgs + [latest_user_message]
        
        # 7. è®¡ç®—æœ€ç»ˆè¯·æ±‚Token
        final_request_tokens = sum(count_tokens(m['content']) for m in final_messages)
        stats = {"request_tokens": final_request_tokens}
        
        logger.info(f"ä¸Šä¸‹æ–‡ç®¡ç†å®Œæˆã€‚æœ€ç»ˆå‘é€ {len(final_messages)} æ¡æ¶ˆæ¯ï¼Œæ€»è¯·æ±‚Tokens: {final_request_tokens}")
        
        return final_messages, stats

    def _generate_and_save_summary(self, text_to_summarize: str, topic_id: int):
        """
        ç”Ÿæˆæ‘˜è¦ï¼Œç°åœ¨è°ƒç”¨è½»é‡çº§æ¥å£ã€‚
        """
        if not text_to_summarize.strip(): return
        
        summary_prompt = prompt_manager.render('summary_prompt.jinja2', dialogue_text=text_to_summarize)
        
        logger.info(f"ä¸º Topic ID {topic_id} ç”Ÿæˆæˆ–æ›´æ–°æ‘˜è¦...")
        try:
            summary_msgs = [{"role": "user", "content": summary_prompt}]
            
            new_summary_raw = self._lightweight_chat_completion(summary_msgs, temperature=0.1)
            if new_summary_raw:
                new_summary = cut_thinking_txt(new_summary_raw)
                db_manager.update_topic_summary(topic_id, new_summary)
                logger.info(f"Topic ID {topic_id} çš„æ‘˜è¦å·²æˆåŠŸç”Ÿæˆæˆ–æ›´æ–°ã€‚")
        except Exception as e:
            logger.error(f"ç”Ÿæˆå’Œä¿å­˜æ‘˜è¦å¤±è´¥: {e}", exc_info=True)

    # --- å„æ¨¡å¼çš„å…·ä½“å®ç° ---

    def _internal_chat_completion(self, messages: List[Dict[str, str]], **kwargs) -> Optional[str]:
        cfg = self.service_config['llm']
        client = OpenAI(base_url=cfg['url'], api_key=cfg['api_key'])
        try:
            completion = client.chat.completions.create(model=cfg['model'], messages=messages, **kwargs)
            # ä½¿ç”¨ getattr å®‰å…¨åœ°è®¿é—®å±æ€§  ä¼ä¸šå†…ç½‘æœåŠ¡å°†æ€è€ƒéƒ¨åˆ†å†…å®¹åˆ†ç¦»äº† response.choices[0].message.reasoning_content + "</think>"
            reasoning = getattr(completion.choices[0].message, 'reasoning_content', None)
            content = getattr(completion.choices[0].message, 'content', '')
            
            if reasoning:
                return f"{reasoning}</think>{content}"
            else:
                return content
            
        except APIError as e:
            logger.error(f"å†…éƒ¨LLM APIé”™è¯¯: {e}")
            raise  # é‡æ–°æŠ›å‡ºï¼Œè®©ä¸Šå±‚æ•è·
        finally:
            client.close()

    def _internal_get_embedding(self, content: Union[str, List[str]], **kwargs) -> Optional[Union[List[float], List[List[float]]]]:
        cfg = self.service_config['embedding']
        client = OpenAI(base_url=cfg['url'], api_key=cfg['api_key'])
        try:
            response = client.embeddings.create(input=content, model=cfg['model'], **kwargs)
            embeddings = [d.embedding for d in response.data]
            return embeddings[0] if isinstance(content, str) else embeddings
        except APIError as e:
            logger.error(f"å†…éƒ¨Embedding APIé”™è¯¯: {e}")
            raise
        finally:
            client.close()

    def _internal_rerank(self, query: str, documents: List[str], **kwargs) -> List[int]:
        cfg = self.service_config['reranker']
        url = f"{cfg['url']}/rerank" # å‡è®¾rerankeræœ‰è‡ªå·±çš„URL
        try:
            # æ„å»ºè¯·æ±‚
            payload = {
                "query": query,
                "documents": documents,
                "model": cfg['model'], 
                "return_documents": True
            }
            
            response = requests.post(url, json=payload, timeout=30)
            response.raise_for_status()
            # è¿”å›çš„æ ¼å¼æ˜¯ {"results": [{"document": "...", "relevance_score": 0.8, "index": 2}]}
            # reranked_results = response.json().get("results", [])
            reranked_results = sorted(response.json()['results'], key=lambda x: x['relevance_score'], reverse=True)
            return [item['index'] for item in reranked_results]
        except Exception as e:
            logger.error(f"å†…éƒ¨Rerankerè°ƒç”¨å¤±è´¥: {e}ã€‚è¿”å›åŸå§‹é¡ºåºã€‚")
            return list(range(len(documents)))
    
    def _external_chat_completion(self, messages: List[Dict[str, str]], **kwargs) -> Optional[str]:
        """ä½¿ç”¨é€‰ä¸­çš„å¤–éƒ¨æä¾›å•†è¿›è¡ŒèŠå¤©è¡¥å…¨"""
        # è·å–é€‰ä¸­çš„èŠå¤©æ¨¡å‹é…ç½®
        selected = config.get_selected_model('chat')
        provider = selected.get('provider')
        model = selected.get('model')

        if not provider or not model:
            logger.error("æœªé…ç½®é€‰ä¸­çš„èŠå¤©æ¨¡å‹")
            return None

        # è·å–å¯¹åº”çš„å®¢æˆ·ç«¯
        client = getattr(self, f'{provider}_client', None)
        if not client:
            logger.error(f"æœªæ‰¾åˆ° {provider} å®¢æˆ·ç«¯")
            return None

        try:
            completion = client.chat.completions.create(model=model, messages=messages, **kwargs)
            return completion.choices[0].message.content
        except APIError as e:
            logger.error(f"å¤–éƒ¨LLM APIé”™è¯¯ ({provider}/{model}): {e}")
            raise

    def _external_get_embedding(self, content: Union[str, List[str]], **kwargs) -> Optional[Union[List[float], List[List[float]]]]:
        """ä½¿ç”¨é€‰ä¸­çš„å¤–éƒ¨æä¾›å•†è¿›è¡Œå‘é‡åŒ–"""
        # è·å–é€‰ä¸­çš„embeddingæ¨¡å‹é…ç½®
        selected = config.get_selected_model('embedding')
        provider = selected.get('provider')
        model = selected.get('model')

        if not provider or not model:
            logger.error("æœªé…ç½®é€‰ä¸­çš„Embeddingæ¨¡å‹")
            return None

        # è·å–å¯¹åº”çš„å®¢æˆ·ç«¯
        client = getattr(self, f'{provider}_client', None)
        if not client:
            logger.error(f"æœªæ‰¾åˆ° {provider} å®¢æˆ·ç«¯")
            return None

        # è·å–æ¨¡å‹çš„æ‰¹å¤„ç†å¤§å°é™åˆ¶
        model_config = config.get_model_config(provider, 'embedding', model)
        batch_size = model_config.get('batch_size', 10)

        try:
            is_single = isinstance(content, str)
            inputs = [content] if is_single else content

            # å¦‚æœè¾“å…¥æ•°é‡è¶…è¿‡é™åˆ¶ï¼Œéœ€è¦åˆ†æ‰¹å¤„ç†
            all_embeddings = []
            for i in range(0, len(inputs), batch_size):
                batch = inputs[i:i + batch_size]
                logger.debug(f"Embeddingæ‰¹å¤„ç†: å¤„ç†ç¬¬ {i//batch_size + 1} æ‰¹ï¼ŒåŒ…å« {len(batch)} ä¸ªæ–‡æœ¬")
                response = client.embeddings.create(input=batch, model=model, **kwargs)
                batch_embeddings = [d.embedding for d in response.data]
                all_embeddings.extend(batch_embeddings)

            return all_embeddings[0] if is_single else all_embeddings
        except APIError as e:
            logger.error(f"å¤–éƒ¨Embedding APIé”™è¯¯ ({provider}/{model}): {e}")
            raise

    def _external_rerank(self, query: str, documents: List[str], **kwargs) -> List[int]:
        """
        ä½¿ç”¨é€‰ä¸­çš„å¤–éƒ¨æä¾›å•†è¿›è¡Œæ–‡æ¡£é‡æ’åº
        æ”¯æŒåŒRerankeræ··æ’ï¼šå¦‚æœä¸¤ä¸ªæ§½ä½éƒ½å¯ç”¨ï¼Œåˆ™å¯¹ä¸¤ä¸ªæ¨¡å‹çš„ç»“æœè¿›è¡ŒåŠ æƒæ··åˆ
        """
        # è·å–æ‰€æœ‰å¯ç”¨çš„rerankeræ§½ä½
        enabled_slots = config.get_enabled_slots('reranker')

        if not enabled_slots:
            logger.warning("æœªé…ç½®å¯ç”¨çš„Rerankeræ§½ä½ï¼Œè¿”å›åŸå§‹é¡ºåº")
            return list(range(len(documents)))

        # å¦‚æœåªæœ‰ä¸€ä¸ªæ§½ä½å¯ç”¨ï¼Œä½¿ç”¨åŸæ¥çš„é€»è¾‘
        if len(enabled_slots) == 1:
            return self._single_slot_rerank(query, documents, enabled_slots[0])
        else:
            # ä¸¤ä¸ªæ§½ä½éƒ½å¯ç”¨ï¼Œä½¿ç”¨æ··æ’é€»è¾‘
            return self._hybrid_rerank(query, documents, enabled_slots)

    def _single_slot_rerank(self, query: str, documents: List[str], slot_num: int) -> List[int]:
        """ä½¿ç”¨å•ä¸ªæ§½ä½çš„rerankerè¿›è¡Œé‡æ’åº"""
        slot_config = config.get_slot_config('reranker', slot_num)
        provider = slot_config.get('provider', '')
        model = slot_config.get('model_name', '')
        base_url = slot_config.get('base_url', '')
        api_key = slot_config.get('api_key', '')

        if not provider or not model:
            logger.warning(f"æ§½ä½ {slot_num} æœªé…ç½®å®Œæ•´çš„Rerankeræ¨¡å‹ï¼Œè¿”å›åŸå§‹é¡ºåº")
            return list(range(len(documents)))

        if not base_url:
            logger.warning(f"æ§½ä½ {slot_num} æœªé…ç½®Base URLï¼Œè¿”å›åŸå§‹é¡ºåº")
            return list(range(len(documents)))

        try:
            headers = {
                "Authorization": f"Bearer {api_key}",
                "Content-Type": "application/json"
            }

            payload = {
                "model": model,
                "query": query,
                "documents": documents,
                "top_n": len(documents)
            }

            response = requests.post(base_url, headers=headers, json=payload, timeout=30)
            response.raise_for_status()

            result = response.json()
            logger.info(f"æ§½ä½{slot_num} rerankæ¶ˆè€—token: {result.get('usage', {}).get('total_tokens', 'N/A')}, æ¨¡å‹: {result.get('model', model)}")

            # è§£æè¿”å›ç»“æœ
            reranked_results = sorted(result.get('results', []), key=lambda x: x.get('relevance_score', 0), reverse=True)
            return [item['index'] for item in reranked_results]

        except Exception as e:
            logger.error(f"æ§½ä½ {slot_num} Reranker APIé”™è¯¯ ({provider}/{model}): {e}ï¼Œè¿”å›åŸå§‹é¡ºåº")
            return list(range(len(documents)))

    def _hybrid_rerank(self, query: str, documents: List[str], slot_nums: List[int]) -> List[int]:
        """
        åŒRerankeræ··æ’ï¼šè°ƒç”¨ä¸¤ä¸ªæ§½ä½çš„rerankerï¼Œå¯¹ç»“æœè¿›è¡ŒåŠ æƒæ··åˆ

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            documents: æ–‡æ¡£åˆ—è¡¨
            slot_nums: å¯ç”¨çš„æ§½ä½ç¼–å·åˆ—è¡¨

        Returns:
            æ··æ’åçš„æ–‡æ¡£ç´¢å¼•åˆ—è¡¨
        """
        # è·å–æ¯ä¸ªæ§½ä½çš„æƒé‡
        weights = {}
        for slot_num in slot_nums:
            slot_config = config.get_slot_config('reranker', slot_num)
            weights[slot_num] = slot_config.get('weight', 0.5)

        # å½’ä¸€åŒ–æƒé‡ï¼Œç¡®ä¿å’Œä¸º1
        total_weight = sum(weights.values())
        if total_weight == 0:
            weights = {slot: 0.5 for slot in slot_nums}
        else:
            weights = {slot: weight / total_weight for slot, weight in weights.items()}

        logger.info(f"ä½¿ç”¨æ··æ’Rerankerï¼Œæ§½ä½æƒé‡: {weights}")

        # è°ƒç”¨æ¯ä¸ªæ§½ä½çš„rerankerï¼Œæ”¶é›†æ¯ä¸ªæ–‡æ¡£çš„ç»¼åˆå¾—åˆ†
        doc_scores = {i: 0.0 for i in range(len(documents))}

        for slot_num in slot_nums:
            slot_results = self._single_slot_rerank_with_scores(query, documents, slot_num)
            weight = weights[slot_num]

            # æ ¹æ®æ’åè®¡ç®—åˆ†æ•°ï¼ˆæ’åè¶Šå‰åˆ†æ•°è¶Šé«˜ï¼‰
            for rank, doc_index in enumerate(slot_results):
                # ä½¿ç”¨æŒ‡æ•°è¡°å‡ï¼šæ’å1å¾—åˆ†1.0ï¼Œæ’å2å¾—åˆ†0.9ï¼Œä»¥æ­¤ç±»æ¨
                score = (1.0 / (1 + rank * 0.1)) * weight
                doc_scores[doc_index] += score

        # æŒ‰ç»¼åˆå¾—åˆ†æ’åº
        sorted_docs = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)
        return [doc_index for doc_index, _ in sorted_docs]

    def _single_slot_rerank_with_scores(self, query: str, documents: List[str], slot_num: int) -> List[int]:
        """
        ä½¿ç”¨å•ä¸ªæ§½ä½çš„rerankerè¿›è¡Œé‡æ’åºï¼Œå¹¶è¿”å›æ’åºç»“æœ

        ä¸ _single_slot_rerank çš„åŒºåˆ«æ˜¯ï¼š
        - _single_slot_rerank ç›´æ¥è°ƒç”¨APIå¹¶è¿”å›ç»“æœ
        - _single_slot_rerank_with_scores åŒæ ·è°ƒç”¨APIï¼Œä½†å¯ä»¥ç”¨äºæ··æ’åœºæ™¯
        """
        return self._single_slot_rerank(query, documents, slot_num)
            
    # def _local_chat_completion(self, messages: List[Dict[str, str]], **kwargs) -> Optional[str]:
    #     url = f"{self.service_config['host']}/api/chat"
    #     payload = {"model": self.service_config['chat_model'], "messages": messages, "stream": False, "options": kwargs}
    #     try:
    #         response = requests.post(url, json=payload, timeout=120)
    #         response.raise_for_status()
    #         return response.json()['message']['content']
    #     except requests.RequestException as e:
    #         logger.error(f"æœ¬åœ°OllamaèŠå¤©è¯·æ±‚å¤±è´¥: {e}")
    #         raise RuntimeError(f"æ— æ³•è¿æ¥åˆ°æœ¬åœ°OllamaæœåŠ¡: {e}") from e
        
    def _local_chat_completion(self, messages: List[Dict[str, str]], **kwargs) -> Optional[str]:
        url = f"{self.service_config['host']}/v1"
        client = OpenAI(base_url=url, api_key="ollama")
        try:
            completion = client.chat.completions.create(model=self.service_config['chat_model'], messages=messages, **kwargs)
            # ä½¿ç”¨ getattr å®‰å…¨åœ°è®¿é—®å±æ€§  
            content = getattr(completion.choices[0].message, 'content', '')
            
            return content
            
        except APIError as e:
            logger.error(f"æœ¬åœ°Ollama APIé”™è¯¯: {e}")
            raise  # é‡æ–°æŠ›å‡ºï¼Œè®©ä¸Šå±‚æ•è·
        finally:
            client.close()

    def _local_get_embedding(self, content: Union[str, List[str]], **kwargs) -> Optional[Union[List[float], List[List[float]]]]:
        url = f"{self.service_config['host']}/api/embeddings"
        is_single = isinstance(content, str)
        inputs = [content] if is_single else content
        embeddings = []
        try:
            for text in inputs:
                payload = {"model": self.service_config['embedding_model'], "prompt": text}
                response = requests.post(url, json=payload, timeout=60)
                response.raise_for_status()
                embeddings.append(response.json()['embedding'])
            return embeddings[0] if is_single else embeddings
        except requests.RequestException as e:
            logger.error(f"æœ¬åœ°Ollama Embeddingè¯·æ±‚å¤±è´¥: {e}")
            raise RuntimeError(f"æ— æ³•è¿æ¥åˆ°æœ¬åœ°Ollama EmbeddingæœåŠ¡: {e}") from e

    def _local_rerank(self, query: str, documents: List[str], **kwargs) -> List[int]:
        # Ollama æ”¯æŒ rerank, ä½†éœ€è¦å•ç‹¬çš„ endpoint/model
        # å‡è®¾æœ‰ /api/rerank è¿™æ ·çš„ endpoint
        url = f"{self.service_config['host']}/api/rerank"
        payload = {"model": self.service_config['reranker_model'], "query": query, "documents": documents}
        try:
            response = requests.post(url, json=payload, timeout=30)
            response.raise_for_status()
            # è¿”å›çš„æ ¼å¼æ˜¯ {"results": [{"document": "...", "relevance_score": 0.8, "index": 2}]}
            reranked_results = sorted(response.json()['results'], key=lambda x: x['relevance_score'], reverse=True)
            return [item['index'] for item in reranked_results]
        except Exception:
            # Ollama çš„ rerank endpoint å¹¶éæ ‡å‡†ï¼Œå¦‚æœå¤±è´¥ï¼Œåˆ™è¿”å›åŸå§‹é¡ºåº
            logger.warning(f"æœ¬åœ°Ollama Rerankerè°ƒç”¨å¤±è´¥æˆ–æœªé…ç½®ã€‚è¿”å›åŸå§‹é¡ºåºã€‚")
            return list(range(len(documents)))

# åˆ›å»ºä¸€ä¸ªå…¨å±€å®ä¾‹
llm_service = LLMService()

